{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE574 Project 1.1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/MicroprocessorX069/FirstRep/blob/master/CSE574_Project_1_1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "58iJsObe0jBm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Importing libraries, dependencies and frameworks\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "q84CZmZMfMwf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kOhjhFTyd-Ob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Software 1.0\n",
        " **fizzbuzz(n)**: #The  function returns either of the 4 classes[\"Fizz\",\"Buzz\",\"FizzBuzz\",\"Other\"] on taking any n as integer input by test of divisibility of 3, 5 or (3 and 5) respectively.\n",
        " If no. is divisible by 3: \"Fizz\"\n",
        " If no. is divisible by 5: \"Buzz\"\n",
        " If no. is divisible by 3 and divisible by 5: \"FizzBuzz\"\n",
        "None of the above: \"Other\"\n",
        " \n",
        " **Input**: An integer \n",
        " \n",
        " **Output**: : A String from [\"Fizz\",\"Buzz\",\"FizzBuzz\",\"Other\"]"
      ]
    },
    {
      "metadata": {
        "id": "hXzCpnBhd8VN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def fizzbuzz(n):\n",
        "    \n",
        "    #The  function returns either of the 4 classes[\"Fizz\",\"Buzz\",\"FizzBuzz\",\"Other\"] on taking any n as integer input.\n",
        "    #It incorporates 3 conditions; divisible by 15 then \"FizzBuzz\", divisible by 3 then\"Fizz\" or divisible by 5 then \"Buzz\" else \"Other\"\n",
        "    # x % y gives remainder when x is divided by y. \n",
        "    \n",
        "    if n % 3 == 0 and n % 5 == 0:\n",
        "        return 'FizzBuzz'\n",
        "    elif n % 3 == 0:\n",
        "        return 'Fizz'\n",
        "    elif n % 5 == 0:\n",
        "        return 'Buzz'\n",
        "    else:\n",
        "        return 'Other'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l2qJkEQudaWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating data set using software 1.0\n"
      ]
    },
    {
      "metadata": {
        "id": "xnYdCarHgEKR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def encodeData(data):\n",
        "    \n",
        "    processedData = []\n",
        "    \n",
        "    for dataInstance in data:\n",
        "        \n",
        " # Why do we have number 10?         \n",
        "      #  Since the maximum no. is the dataset is 999, whole binary form is -\n",
        "      #  0b1111100111, i.e. it requires 10 digits to store the number.\n",
        "      \n",
        "      #  dataInstance is right shifted 10 times and the right most bit is  -    # x >> y Shifts x by y bits right side. Basically x // (2**y) \n",
        "      #  appended to the dataset by 'and'ing with index of the digit to be -    # //  gives the whole number  of x/y. i.e. 10//3 is 3 since 10/3 is 3.333.\n",
        "      #  extracted\n",
        "        processedData.append([dataInstance >> d & 1 for d in range(10)])\n",
        "    \n",
        "    return np.array(processedData)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "\n",
        "def encodeOutput(y):\n",
        "  processed_y=[]\n",
        "                                                                                 \n",
        "  for y_label in y:\n",
        "    if(y_label==\"Fizz\"):\n",
        "      processed_y.append([0])\n",
        "    elif (y_label==\"Buzz\"):\n",
        "      processed_y.append([1])\n",
        "    elif (y_label==\"FizzBuzz\"):\n",
        "      processed_y.append([2])\n",
        "    else:\n",
        "      processed_y.append([3])\n",
        "      \n",
        "  return(np_utils.to_categorical(np.array(processed_y),4))  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f16fOQzjdZnS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def createDataset(start,end):\n",
        "    \n",
        "    # Why list in Python?                                                       #Lists in python support varied function for easy retrieval of data. \n",
        "                                                                                #They are compatible with loops and supports varied indexing. \n",
        "                                                                                #In this problem, since the data is simple, integer type lists would be the best option for minimal computaion.\n",
        "\n",
        "    inputData   = []\n",
        "    outputData  = []\n",
        "    \n",
        "    # Why do we need training Data?                                             #Back propogation algorithm is a supervised learning algorithm. \n",
        "                                                                                #Training dataset is important for the model to learn from by regularly calculating the error function and updating the weights. This makes the model more accurate towards the intended result\n",
        "    \n",
        "    for i in range(start,end):\n",
        "        inputData.append(i)\n",
        "        outputData.append(fizzbuzz(i))\n",
        "    \n",
        "   \n",
        "    processedData  = encodeData(inputData)\n",
        "    processedLabel = encodeOutput(outputData)\n",
        "    \n",
        "    return processedData, processedLabel\n",
        "    \n",
        "   \n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W04MU0DYQnBr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "Functions definitions:\n",
        "1. **createDataset(start, end)**: a. a. Segregates the dataset into input and output data. b. Processes input data interms of binary digits as individual features of thhe dataset. c. Processes output labelled data into factors as required by the algorithm. (Binary class factorization in this case . Example Suppose an output belongs to 2nd class out of 0, 1, 2, 3, it will be represented as 0 0 1 0)\n",
        "\n",
        "**Input**: dataset: Multi data type dataframe. Ex. [ [ 101, 'Other' ], [ 103, 'Fizz' ] ]  for three classes (Fizz(0), Buzz(1), FizzBuzz(2), Other(3))\n",
        "\n",
        "**Output**: Two datasets of binary input and output lists of repective lengths.  Ex. processedData=[ [1,0,1,0,0,1,1,0,0,0] , [1,0,1,1,0,1,1,0,0,0] ]; processedLabel [ [0,0,0,1], [1,0,0,0] ].\n",
        "  \n",
        "*///Example. of  processData([101, Other]) illustrated within the code as comments, stepwise.*\n",
        "\n",
        "2. **encodeData(data)**: Converts a given list of integer numbers to binary bits and stores each bit as an element of row of a dataframe.\n",
        "\n",
        "**Input**: dataset: Integer data type list. Ex. [ 101, 103 , 104 ]\n",
        "\n",
        "**Output**: datasets of binary 2D list( no. of columns defined in the function).  Ex. processedData=[ [1,0,1,0,0,1,1,0,0,0] , [1,0,1,1,0,1,1,0,0,0], [1,0,0,1,0,1,1,0,0] ]\n",
        "\n",
        "3. **encodeLabel(data)**: Converts a given list of factor(Any datatype) to list of binary bits (length= number of unique classes).\n",
        "\n",
        "**Input**:  List. Ex. [ 'Other', 'Fizz', 'Buzz' ]\n",
        "\n",
        "**Output**: List of integer (1 or O) ( Length is the total no. of unique classes in the dataset).  Ex. [ [0,0,0,1], [1,0,0,0], [0,1,0,0] ]\n",
        "\n",
        "\n",
        "                 "
      ]
    },
    {
      "metadata": {
        "id": "e2CQ9D7Tc1Cp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing data set from Google drive/cloud**\n",
        "\n",
        "This part of the code is for running the model on Google colaboratory.\n",
        "For authentication."
      ]
    },
    {
      "metadata": {
        "id": "qxkhOdVujN_h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "c417f03b-6508-4e9d-b395-b319e98a2992"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Code to authenticate the user to let the model access google drive and cloud. \n",
        "#For importing datasets from google drive\n",
        "#[1]\n",
        "\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "'''"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Code to authenticate the user to let the model access google drive and cloud. \\n#For importing datasets from google drive\\n#[1]\\n\\n!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\\n!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\\n!apt-get update -qq 2>&1 > /dev/null\\n!apt-get -y install -qq google-drive-ocamlfuse fuse\\nfrom google.colab import auth\\nauth.authenticate_user()\\nfrom oauth2client.client import GoogleCredentials\\ncreds = GoogleCredentials.get_application_default()\\nimport getpass\\n!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\\nvcode = getpass.getpass()\\n!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "jFCyS7m9dP2p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mounting Google drive for importing the data."
      ]
    },
    {
      "metadata": {
        "id": "hJEh3udjKW2I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8726ea41-d2bc-4f17-9dc5-dec9e4b14a80"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#[1]\n",
        "#!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive\n",
        "'''\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#[1]\\n#!mkdir -p drive\\n!google-drive-ocamlfuse drive\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "CPFziK5PdV9d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "___\n",
        "**Segregating the test and train data**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "5vjzQgCTjGcd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "### THe below commented code accesses the files from google drive from the given path. This is subjective to the respective google account.\n",
        "#train_df=pd.read_csv('drive/path/to/your/Google driv folder/training.csv') #th path start with \"drive/\"\n",
        "#test_df=pd.read_csv('drive/path/to/your/Google driv folder/testing.csv')\n",
        "\n",
        "# Calling functions to create, segregate datasets into train and test.\n",
        "#The target labels of the datasets are also factorized.\n",
        "\n",
        "processedData, processedLabel = createDataset(101,1000) # processedData and processedLabel are the input features and target values of the train dataset.\n",
        "                                                        # The train data has input integers from 101 to 999(inclusive).\n",
        "processedTestData,processedTestLabel=createDataset(1,101) # processedData and processedLabel are the input features and target values of the train dataset.\n",
        "                                                          # The test data has input integers from 1 to 100 (inclusive).\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1ycfpj2hZa7n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model parameters\n",
        "Initializing model parameters for tuning the model via an interactive real-time form. Following are the parameters:\n",
        "\n",
        "1. No. of hidden layers. (Name: no_of_layers, Type: Integer, Range: N.A. (feasible for computation) )\n",
        "2. No. of nodes in each hidden layer (Name: nodes_in_layer[i], Type: Integer, Range: N.A. (feasible for computation) )\n",
        "3. Validation split of the dataset( in terms of per unit) (Name: validation_splitt, Type: Float, Range: (0,1))\n",
        "4. Activation Function: (Name: Activation_function, Select: [tf.nn.relu, tf.nn.relu6, tf.nn.crelu, tf.nn.elu, tf.nn.selu, tf.nn.softplus, tf.nn.softsign, tf.nn.dropout, tf.nn.bias_add, tf.sigmoid, tf.tanh])\n",
        "5. Loss Function (Name: loss_function, Select: \"categorical_crossentropy\", \"sparse_categorical_crossentropy\", \"binary_crossentropy\",\"kullback_leibler_divergence\",\"poisson\",\"cosine_proximity\" )\n",
        "6. Optimizer : (Name: optimizer_used, Select: \"SGD\", \"RMSprop\", \"Adagrad\", \"Adadelta\", \"Adam\", \" Adamax\", \"Nadam\", \"TFOptimizer\")\n",
        "\n",
        "[2]"
      ]
    },
    {
      "metadata": {
        "id": "heSgWPmmj4DB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Parameters for the model { run: \"auto\" } \n",
        "\n",
        "import tensorflow as tf\n",
        "#run:auto command makes the code block run in real time. Whenever the user amends the input, the block runs automatically for the new set of input.\n",
        "no_of_layers= 3 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "# A is the list of nodes_in_each_hidden_layer for each of the user defined hidden layers. Eg. A=[128,256,64] implies 3 hidden layers with 128 nodes, 256 nodes and 64 nodes respectively starting from the input layer to the output layer.\n",
        "A=[]\n",
        "# No. of nodes in hidden layer 1\n",
        "nodes_in_layer1 = 64 #@param {type:\"integer\"}\n",
        "A.append(nodes_in_layer1)\n",
        "#dropout_rate = 0.1 #@param {min:0.000,max:1.000}\n",
        "\n",
        "nodes_in_layer2 = 256 #@param {type:\"integer\"}\n",
        "A.append(nodes_in_layer2)\n",
        "nodes_in_layer3 = 128 #@param {type:\"integer\"}\n",
        "A.append(nodes_in_layer3)\n",
        "validation_splitt=0.2 #@param{type:\"slider\",min:0.000,max:1.000,step:0.05}\n",
        "Activation_function = tf.nn.crelu #@param [\"relu\", \"relu6\", \"crelu\", \"elu\", \"selu\", \"softplus\", \"softsign\", \"dropout\", \"bias_add\", \"sigmoid\", \"tanh\"]\n",
        "loss_function = 'kullback_leibler_divergence' #@param [\"categorical_crossentropy\", \"sparse_categorical_crossentropy\", \"binary_crossentropy\",\"kullback_leibler_divergence\",\"poisson\",\"cosine_proximity\"]\n",
        "optimizer_used = 'SGD' #@param [\"SGD\", \"RMSprop\", \"Adagrad\",\"Adadelta\",\"Adam\",\"Adamax\",\"Nadam\",\"TFOptimizer\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B4Nj4ctRsguu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Model Creation**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Ss0GrF5pslAb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34927
        },
        "cellView": "both",
        "outputId": "1f6e4cb4-82d8-4d0b-c926-5af26c90a0c6"
      },
      "cell_type": "code",
      "source": [
        "#@title Parametres for the model { run: \"auto\" }\n",
        "\n",
        "#no_epochs is the integer input for total no. of model parsing -\n",
        "#from input to generating output and back to updating the weights -\n",
        "#according to the error for all training examples.\n",
        "\n",
        "no_epochs=1000 #@param{type:\"slider\",min:0,max:10000,step:200}\n",
        " \n",
        "import time\n",
        "start=time.time()\n",
        "\n",
        "#Libraries for current codeblock\n",
        "import tensorflow as tf\n",
        "from tensorflow import nn\n",
        "nodes_in_output_layer=4\n",
        "input_size=10\n",
        "\n",
        "model=tf.keras.models.Sequential()\n",
        "\n",
        "#Various types of layers in keras: Dense, Flatten, Input, Permute, Reshape.[3]\n",
        "#Dense implements the operation: output = activation(dot(input, kernel) + bias)[3]\n",
        "\n",
        "#Adding input layer with size of input i.e. 10 in this case. \n",
        "#The first parameter for function Dense() is No. of nodes. in the layer. model.add appends the newly created hidden layer to the model.\n",
        "\n",
        "model.add(tf.keras.layers.Dense(A[0],activation=Activation_function,input_dim=input_size))\n",
        "for layer_iterator in range(1,len(A)-1):\n",
        "  model.add(tf.keras.layers.Dense(A[layer_iterator+1], activation=Activation_function))\n",
        "#Creating dropout layer\n",
        "#model.add(tf.keras.layers.Dropout(rate=dropout_rate))\n",
        "# Creating the output layer with 4 nodes. Since Output layer \n",
        "model.add(tf.keras.layers.Dense(nodes_in_output_layer, activation=tf.nn.softmax))\n",
        "\n",
        "# Parameters for model creation i.e. optimizers, loss function\n",
        "                                                                                # metric: Accuracy. Accuracy is the measure of correctness of the model\n",
        "                                                                                # - , basically rate to which it classifies the inputs correctly.\n",
        "                                                                                # Validation Accuracy: Percentage of correctly classifying on input chosen from the train dataset itself.\n",
        "                                                                                # Testing Accuracy:  Percentage of correctly classifying on input from a new dataset following the same problem as the train dataset.\n",
        "\n",
        "                                                                                # Optimizer: Algorithm to minimize the error/loss by updating weights to improve the training of the model. e.g.\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function, metrics=['accuracy'])\n",
        "model.summary()\n",
        "history=model.fit(processedData, processedLabel, epochs= no_epochs, validation_split= validation_splitt)\n",
        "\n",
        "end=time.time()\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 128)               704       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 256)               16512     \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 18,244\n",
            "Trainable params: 18,244\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 719 samples, validate on 180 samples\n",
            "Epoch 1/1000\n",
            "719/719 [==============================] - 0s 284us/step - loss: 1.2623 - acc: 0.4951 - val_loss: 1.1779 - val_acc: 0.5333\n",
            "Epoch 2/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1797 - acc: 0.5341 - val_loss: 1.1494 - val_acc: 0.5333\n",
            "Epoch 3/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1592 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 4/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1525 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 5/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1490 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 6/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1476 - acc: 0.5341 - val_loss: 1.1424 - val_acc: 0.5333\n",
            "Epoch 7/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1463 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 8/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1457 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 9/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1452 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 10/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1438 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 11/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1436 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 12/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1427 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 13/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1423 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 14/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1416 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 15/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1415 - acc: 0.5341 - val_loss: 1.1446 - val_acc: 0.5333\n",
            "Epoch 16/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 1.1407 - acc: 0.5341 - val_loss: 1.1442 - val_acc: 0.5333\n",
            "Epoch 17/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1404 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 18/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1398 - acc: 0.5341 - val_loss: 1.1437 - val_acc: 0.5333\n",
            "Epoch 19/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1400 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 20/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1395 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 21/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1389 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 22/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1383 - acc: 0.5341 - val_loss: 1.1417 - val_acc: 0.5333\n",
            "Epoch 23/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1379 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 24/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1381 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 25/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 1.1378 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 26/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1367 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 27/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1370 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 28/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1365 - acc: 0.5341 - val_loss: 1.1457 - val_acc: 0.5333\n",
            "Epoch 29/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1362 - acc: 0.5341 - val_loss: 1.1420 - val_acc: 0.5333\n",
            "Epoch 30/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1359 - acc: 0.5341 - val_loss: 1.1416 - val_acc: 0.5333\n",
            "Epoch 31/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1354 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 32/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1353 - acc: 0.5341 - val_loss: 1.1427 - val_acc: 0.5333\n",
            "Epoch 33/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1350 - acc: 0.5341 - val_loss: 1.1417 - val_acc: 0.5333\n",
            "Epoch 34/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1350 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 35/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 1.1338 - acc: 0.5341 - val_loss: 1.1415 - val_acc: 0.5333\n",
            "Epoch 36/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1338 - acc: 0.5341 - val_loss: 1.1432 - val_acc: 0.5333\n",
            "Epoch 37/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1340 - acc: 0.5341 - val_loss: 1.1438 - val_acc: 0.5333\n",
            "Epoch 38/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1332 - acc: 0.5341 - val_loss: 1.1418 - val_acc: 0.5333\n",
            "Epoch 39/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1331 - acc: 0.5341 - val_loss: 1.1418 - val_acc: 0.5333\n",
            "Epoch 40/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1330 - acc: 0.5341 - val_loss: 1.1411 - val_acc: 0.5333\n",
            "Epoch 41/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1327 - acc: 0.5341 - val_loss: 1.1410 - val_acc: 0.5333\n",
            "Epoch 42/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1322 - acc: 0.5341 - val_loss: 1.1413 - val_acc: 0.5333\n",
            "Epoch 43/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1321 - acc: 0.5341 - val_loss: 1.1409 - val_acc: 0.5333\n",
            "Epoch 44/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1319 - acc: 0.5341 - val_loss: 1.1405 - val_acc: 0.5333\n",
            "Epoch 45/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1317 - acc: 0.5341 - val_loss: 1.1411 - val_acc: 0.5333\n",
            "Epoch 46/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1321 - acc: 0.5341 - val_loss: 1.1412 - val_acc: 0.5333\n",
            "Epoch 47/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1311 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 48/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1308 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 49/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1305 - acc: 0.5341 - val_loss: 1.1414 - val_acc: 0.5333\n",
            "Epoch 50/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1304 - acc: 0.5341 - val_loss: 1.1415 - val_acc: 0.5333\n",
            "Epoch 51/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1307 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 52/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1298 - acc: 0.5341 - val_loss: 1.1421 - val_acc: 0.5333\n",
            "Epoch 53/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1293 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 54/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1296 - acc: 0.5341 - val_loss: 1.1435 - val_acc: 0.5333\n",
            "Epoch 55/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1294 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 56/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1289 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 57/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1292 - acc: 0.5341 - val_loss: 1.1420 - val_acc: 0.5333\n",
            "Epoch 58/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1284 - acc: 0.5341 - val_loss: 1.1416 - val_acc: 0.5333\n",
            "Epoch 59/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1283 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 60/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.1285 - acc: 0.5341 - val_loss: 1.1420 - val_acc: 0.5333\n",
            "Epoch 61/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1276 - acc: 0.5341 - val_loss: 1.1417 - val_acc: 0.5333\n",
            "Epoch 62/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1274 - acc: 0.5341 - val_loss: 1.1424 - val_acc: 0.5333\n",
            "Epoch 63/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1276 - acc: 0.5341 - val_loss: 1.1431 - val_acc: 0.5333\n",
            "Epoch 64/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1273 - acc: 0.5341 - val_loss: 1.1417 - val_acc: 0.5333\n",
            "Epoch 65/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1268 - acc: 0.5341 - val_loss: 1.1424 - val_acc: 0.5333\n",
            "Epoch 66/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1267 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 67/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1266 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 68/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1260 - acc: 0.5341 - val_loss: 1.1422 - val_acc: 0.5333\n",
            "Epoch 69/1000\n",
            "719/719 [==============================] - 0s 69us/step - loss: 1.1259 - acc: 0.5341 - val_loss: 1.1415 - val_acc: 0.5333\n",
            "Epoch 70/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1260 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 71/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1260 - acc: 0.5341 - val_loss: 1.1421 - val_acc: 0.5333\n",
            "Epoch 72/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1256 - acc: 0.5341 - val_loss: 1.1424 - val_acc: 0.5333\n",
            "Epoch 73/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1252 - acc: 0.5341 - val_loss: 1.1412 - val_acc: 0.5333\n",
            "Epoch 74/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 1.1252 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 75/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1245 - acc: 0.5341 - val_loss: 1.1420 - val_acc: 0.5333\n",
            "Epoch 76/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1246 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 77/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1242 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 78/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1239 - acc: 0.5341 - val_loss: 1.1420 - val_acc: 0.5333\n",
            "Epoch 79/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1240 - acc: 0.5341 - val_loss: 1.1415 - val_acc: 0.5333\n",
            "Epoch 80/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 1.1234 - acc: 0.5341 - val_loss: 1.1431 - val_acc: 0.5333\n",
            "Epoch 81/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1233 - acc: 0.5341 - val_loss: 1.1448 - val_acc: 0.5333\n",
            "Epoch 82/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1231 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 83/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1233 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 84/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 1.1225 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 85/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.1227 - acc: 0.5341 - val_loss: 1.1436 - val_acc: 0.5333\n",
            "Epoch 86/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 1.1228 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 87/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1221 - acc: 0.5341 - val_loss: 1.1425 - val_acc: 0.5333\n",
            "Epoch 88/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.1223 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 89/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 1.1218 - acc: 0.5341 - val_loss: 1.1415 - val_acc: 0.5333\n",
            "Epoch 90/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1215 - acc: 0.5341 - val_loss: 1.1418 - val_acc: 0.5333\n",
            "Epoch 91/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 1.1211 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 92/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 1.1210 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 93/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 1.1208 - acc: 0.5341 - val_loss: 1.1436 - val_acc: 0.5333\n",
            "Epoch 94/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1201 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 95/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.1208 - acc: 0.5341 - val_loss: 1.1427 - val_acc: 0.5333\n",
            "Epoch 96/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1203 - acc: 0.5341 - val_loss: 1.1430 - val_acc: 0.5333\n",
            "Epoch 97/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1197 - acc: 0.5341 - val_loss: 1.1427 - val_acc: 0.5333\n",
            "Epoch 98/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1196 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 99/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1194 - acc: 0.5341 - val_loss: 1.1425 - val_acc: 0.5333\n",
            "Epoch 100/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1193 - acc: 0.5341 - val_loss: 1.1421 - val_acc: 0.5333\n",
            "Epoch 101/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1188 - acc: 0.5341 - val_loss: 1.1418 - val_acc: 0.5333\n",
            "Epoch 102/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1190 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 103/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1187 - acc: 0.5341 - val_loss: 1.1430 - val_acc: 0.5333\n",
            "Epoch 104/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1180 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 105/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1184 - acc: 0.5341 - val_loss: 1.1431 - val_acc: 0.5333\n",
            "Epoch 106/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1182 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 107/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1177 - acc: 0.5341 - val_loss: 1.1430 - val_acc: 0.5333\n",
            "Epoch 108/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 1.1175 - acc: 0.5341 - val_loss: 1.1423 - val_acc: 0.5333\n",
            "Epoch 109/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1177 - acc: 0.5341 - val_loss: 1.1419 - val_acc: 0.5333\n",
            "Epoch 110/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1170 - acc: 0.5341 - val_loss: 1.1425 - val_acc: 0.5333\n",
            "Epoch 111/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1168 - acc: 0.5341 - val_loss: 1.1424 - val_acc: 0.5333\n",
            "Epoch 112/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1165 - acc: 0.5341 - val_loss: 1.1422 - val_acc: 0.5333\n",
            "Epoch 113/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1162 - acc: 0.5341 - val_loss: 1.1421 - val_acc: 0.5333\n",
            "Epoch 114/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1161 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 115/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1162 - acc: 0.5341 - val_loss: 1.1425 - val_acc: 0.5333\n",
            "Epoch 116/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1156 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 117/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1154 - acc: 0.5341 - val_loss: 1.1426 - val_acc: 0.5333\n",
            "Epoch 118/1000\n",
            "719/719 [==============================] - 0s 65us/step - loss: 1.1149 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 119/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1146 - acc: 0.5341 - val_loss: 1.1442 - val_acc: 0.5333\n",
            "Epoch 120/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 1.1151 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 121/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1143 - acc: 0.5341 - val_loss: 1.1432 - val_acc: 0.5333\n",
            "Epoch 122/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1140 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 123/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1136 - acc: 0.5341 - val_loss: 1.1437 - val_acc: 0.5333\n",
            "Epoch 124/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1139 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 125/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1137 - acc: 0.5341 - val_loss: 1.1435 - val_acc: 0.5333\n",
            "Epoch 126/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1131 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 127/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 1.1132 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 128/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1127 - acc: 0.5341 - val_loss: 1.1437 - val_acc: 0.5333\n",
            "Epoch 129/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1127 - acc: 0.5341 - val_loss: 1.1429 - val_acc: 0.5333\n",
            "Epoch 130/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1123 - acc: 0.5341 - val_loss: 1.1431 - val_acc: 0.5333\n",
            "Epoch 131/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1123 - acc: 0.5341 - val_loss: 1.1428 - val_acc: 0.5333\n",
            "Epoch 132/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1118 - acc: 0.5341 - val_loss: 1.1434 - val_acc: 0.5333\n",
            "Epoch 133/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1114 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 134/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1111 - acc: 0.5341 - val_loss: 1.1436 - val_acc: 0.5333\n",
            "Epoch 135/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1109 - acc: 0.5341 - val_loss: 1.1433 - val_acc: 0.5333\n",
            "Epoch 136/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1100 - acc: 0.5341 - val_loss: 1.1439 - val_acc: 0.5333\n",
            "Epoch 137/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1101 - acc: 0.5341 - val_loss: 1.1435 - val_acc: 0.5333\n",
            "Epoch 138/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1103 - acc: 0.5341 - val_loss: 1.1437 - val_acc: 0.5333\n",
            "Epoch 139/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1100 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 140/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1097 - acc: 0.5341 - val_loss: 1.1439 - val_acc: 0.5333\n",
            "Epoch 141/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1097 - acc: 0.5341 - val_loss: 1.1440 - val_acc: 0.5333\n",
            "Epoch 142/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1093 - acc: 0.5341 - val_loss: 1.1438 - val_acc: 0.5333\n",
            "Epoch 143/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1090 - acc: 0.5341 - val_loss: 1.1446 - val_acc: 0.5333\n",
            "Epoch 144/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.1092 - acc: 0.5341 - val_loss: 1.1448 - val_acc: 0.5333\n",
            "Epoch 145/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1090 - acc: 0.5341 - val_loss: 1.1447 - val_acc: 0.5333\n",
            "Epoch 146/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1080 - acc: 0.5341 - val_loss: 1.1442 - val_acc: 0.5333\n",
            "Epoch 147/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.1082 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 148/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1077 - acc: 0.5341 - val_loss: 1.1438 - val_acc: 0.5333\n",
            "Epoch 149/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.1071 - acc: 0.5341 - val_loss: 1.1443 - val_acc: 0.5333\n",
            "Epoch 150/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 1.1070 - acc: 0.5341 - val_loss: 1.1445 - val_acc: 0.5333\n",
            "Epoch 151/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1065 - acc: 0.5341 - val_loss: 1.1443 - val_acc: 0.5333\n",
            "Epoch 152/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1066 - acc: 0.5341 - val_loss: 1.1444 - val_acc: 0.5333\n",
            "Epoch 153/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.1063 - acc: 0.5341 - val_loss: 1.1448 - val_acc: 0.5333\n",
            "Epoch 154/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1061 - acc: 0.5341 - val_loss: 1.1451 - val_acc: 0.5333\n",
            "Epoch 155/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1062 - acc: 0.5341 - val_loss: 1.1449 - val_acc: 0.5333\n",
            "Epoch 156/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1054 - acc: 0.5341 - val_loss: 1.1454 - val_acc: 0.5333\n",
            "Epoch 157/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1057 - acc: 0.5341 - val_loss: 1.1452 - val_acc: 0.5333\n",
            "Epoch 158/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1051 - acc: 0.5341 - val_loss: 1.1455 - val_acc: 0.5333\n",
            "Epoch 159/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.1053 - acc: 0.5341 - val_loss: 1.1448 - val_acc: 0.5333\n",
            "Epoch 160/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1040 - acc: 0.5341 - val_loss: 1.1457 - val_acc: 0.5333\n",
            "Epoch 161/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1047 - acc: 0.5341 - val_loss: 1.1456 - val_acc: 0.5333\n",
            "Epoch 162/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1040 - acc: 0.5341 - val_loss: 1.1458 - val_acc: 0.5333\n",
            "Epoch 163/1000\n",
            "719/719 [==============================] - 0s 65us/step - loss: 1.1032 - acc: 0.5341 - val_loss: 1.1461 - val_acc: 0.5333\n",
            "Epoch 164/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1029 - acc: 0.5341 - val_loss: 1.1454 - val_acc: 0.5333\n",
            "Epoch 165/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1027 - acc: 0.5341 - val_loss: 1.1455 - val_acc: 0.5333\n",
            "Epoch 166/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.1028 - acc: 0.5341 - val_loss: 1.1454 - val_acc: 0.5333\n",
            "Epoch 167/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.1025 - acc: 0.5341 - val_loss: 1.1455 - val_acc: 0.5333\n",
            "Epoch 168/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1023 - acc: 0.5341 - val_loss: 1.1454 - val_acc: 0.5333\n",
            "Epoch 169/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.1020 - acc: 0.5341 - val_loss: 1.1455 - val_acc: 0.5333\n",
            "Epoch 170/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.1017 - acc: 0.5341 - val_loss: 1.1454 - val_acc: 0.5333\n",
            "Epoch 171/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.1014 - acc: 0.5341 - val_loss: 1.1456 - val_acc: 0.5333\n",
            "Epoch 172/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.1011 - acc: 0.5341 - val_loss: 1.1459 - val_acc: 0.5333\n",
            "Epoch 173/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.1005 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 174/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.1004 - acc: 0.5341 - val_loss: 1.1460 - val_acc: 0.5333\n",
            "Epoch 175/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0999 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 176/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0999 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 177/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0994 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 178/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0993 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 179/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0996 - acc: 0.5341 - val_loss: 1.1464 - val_acc: 0.5333\n",
            "Epoch 180/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0985 - acc: 0.5341 - val_loss: 1.1467 - val_acc: 0.5333\n",
            "Epoch 181/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0985 - acc: 0.5341 - val_loss: 1.1462 - val_acc: 0.5333\n",
            "Epoch 182/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0979 - acc: 0.5341 - val_loss: 1.1465 - val_acc: 0.5333\n",
            "Epoch 183/1000\n",
            "719/719 [==============================] - 0s 66us/step - loss: 1.0976 - acc: 0.5341 - val_loss: 1.1465 - val_acc: 0.5333\n",
            "Epoch 184/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0972 - acc: 0.5341 - val_loss: 1.1467 - val_acc: 0.5333\n",
            "Epoch 185/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0970 - acc: 0.5341 - val_loss: 1.1470 - val_acc: 0.5333\n",
            "Epoch 186/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0963 - acc: 0.5341 - val_loss: 1.1480 - val_acc: 0.5333\n",
            "Epoch 187/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0969 - acc: 0.5341 - val_loss: 1.1481 - val_acc: 0.5333\n",
            "Epoch 188/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0962 - acc: 0.5341 - val_loss: 1.1472 - val_acc: 0.5333\n",
            "Epoch 189/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0961 - acc: 0.5341 - val_loss: 1.1473 - val_acc: 0.5333\n",
            "Epoch 190/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 1.0955 - acc: 0.5341 - val_loss: 1.1474 - val_acc: 0.5333\n",
            "Epoch 191/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0952 - acc: 0.5341 - val_loss: 1.1473 - val_acc: 0.5333\n",
            "Epoch 192/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0948 - acc: 0.5341 - val_loss: 1.1481 - val_acc: 0.5333\n",
            "Epoch 193/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0947 - acc: 0.5341 - val_loss: 1.1477 - val_acc: 0.5333\n",
            "Epoch 194/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0945 - acc: 0.5341 - val_loss: 1.1477 - val_acc: 0.5333\n",
            "Epoch 195/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0943 - acc: 0.5341 - val_loss: 1.1482 - val_acc: 0.5333\n",
            "Epoch 196/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0937 - acc: 0.5341 - val_loss: 1.1485 - val_acc: 0.5333\n",
            "Epoch 197/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0935 - acc: 0.5341 - val_loss: 1.1479 - val_acc: 0.5333\n",
            "Epoch 198/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0930 - acc: 0.5341 - val_loss: 1.1478 - val_acc: 0.5333\n",
            "Epoch 199/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0924 - acc: 0.5341 - val_loss: 1.1492 - val_acc: 0.5333\n",
            "Epoch 200/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0926 - acc: 0.5341 - val_loss: 1.1488 - val_acc: 0.5333\n",
            "Epoch 201/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0914 - acc: 0.5341 - val_loss: 1.1482 - val_acc: 0.5333\n",
            "Epoch 202/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0912 - acc: 0.5341 - val_loss: 1.1498 - val_acc: 0.5333\n",
            "Epoch 203/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0912 - acc: 0.5341 - val_loss: 1.1490 - val_acc: 0.5333\n",
            "Epoch 204/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0907 - acc: 0.5341 - val_loss: 1.1495 - val_acc: 0.5333\n",
            "Epoch 205/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0909 - acc: 0.5341 - val_loss: 1.1487 - val_acc: 0.5333\n",
            "Epoch 206/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0909 - acc: 0.5341 - val_loss: 1.1486 - val_acc: 0.5333\n",
            "Epoch 207/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0897 - acc: 0.5341 - val_loss: 1.1492 - val_acc: 0.5333\n",
            "Epoch 208/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0895 - acc: 0.5341 - val_loss: 1.1494 - val_acc: 0.5333\n",
            "Epoch 209/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0891 - acc: 0.5341 - val_loss: 1.1497 - val_acc: 0.5333\n",
            "Epoch 210/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0892 - acc: 0.5341 - val_loss: 1.1495 - val_acc: 0.5333\n",
            "Epoch 211/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0883 - acc: 0.5341 - val_loss: 1.1500 - val_acc: 0.5333\n",
            "Epoch 212/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0880 - acc: 0.5341 - val_loss: 1.1495 - val_acc: 0.5333\n",
            "Epoch 213/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0881 - acc: 0.5341 - val_loss: 1.1494 - val_acc: 0.5333\n",
            "Epoch 214/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0875 - acc: 0.5341 - val_loss: 1.1494 - val_acc: 0.5333\n",
            "Epoch 215/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0870 - acc: 0.5341 - val_loss: 1.1499 - val_acc: 0.5333\n",
            "Epoch 216/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0865 - acc: 0.5341 - val_loss: 1.1496 - val_acc: 0.5333\n",
            "Epoch 217/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0859 - acc: 0.5341 - val_loss: 1.1497 - val_acc: 0.5333\n",
            "Epoch 218/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0861 - acc: 0.5341 - val_loss: 1.1497 - val_acc: 0.5333\n",
            "Epoch 219/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0856 - acc: 0.5341 - val_loss: 1.1498 - val_acc: 0.5333\n",
            "Epoch 220/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0846 - acc: 0.5341 - val_loss: 1.1496 - val_acc: 0.5333\n",
            "Epoch 221/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0849 - acc: 0.5341 - val_loss: 1.1499 - val_acc: 0.5333\n",
            "Epoch 222/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0849 - acc: 0.5341 - val_loss: 1.1500 - val_acc: 0.5333\n",
            "Epoch 223/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0841 - acc: 0.5341 - val_loss: 1.1507 - val_acc: 0.5333\n",
            "Epoch 224/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0837 - acc: 0.5341 - val_loss: 1.1508 - val_acc: 0.5333\n",
            "Epoch 225/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0824 - acc: 0.5341 - val_loss: 1.1503 - val_acc: 0.5333\n",
            "Epoch 226/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0826 - acc: 0.5341 - val_loss: 1.1506 - val_acc: 0.5333\n",
            "Epoch 227/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0821 - acc: 0.5341 - val_loss: 1.1508 - val_acc: 0.5333\n",
            "Epoch 228/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0813 - acc: 0.5341 - val_loss: 1.1515 - val_acc: 0.5278\n",
            "Epoch 229/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0816 - acc: 0.5341 - val_loss: 1.1511 - val_acc: 0.5278\n",
            "Epoch 230/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0808 - acc: 0.5341 - val_loss: 1.1500 - val_acc: 0.5333\n",
            "Epoch 231/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0802 - acc: 0.5355 - val_loss: 1.1528 - val_acc: 0.5278\n",
            "Epoch 232/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0808 - acc: 0.5341 - val_loss: 1.1508 - val_acc: 0.5278\n",
            "Epoch 233/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0799 - acc: 0.5355 - val_loss: 1.1504 - val_acc: 0.5333\n",
            "Epoch 234/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0793 - acc: 0.5341 - val_loss: 1.1505 - val_acc: 0.5333\n",
            "Epoch 235/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0789 - acc: 0.5341 - val_loss: 1.1508 - val_acc: 0.5333\n",
            "Epoch 236/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0789 - acc: 0.5341 - val_loss: 1.1505 - val_acc: 0.5333\n",
            "Epoch 237/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0778 - acc: 0.5341 - val_loss: 1.1516 - val_acc: 0.5278\n",
            "Epoch 238/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0774 - acc: 0.5369 - val_loss: 1.1509 - val_acc: 0.5278\n",
            "Epoch 239/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0774 - acc: 0.5341 - val_loss: 1.1517 - val_acc: 0.5278\n",
            "Epoch 240/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0765 - acc: 0.5341 - val_loss: 1.1507 - val_acc: 0.5278\n",
            "Epoch 241/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0763 - acc: 0.5341 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 242/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0753 - acc: 0.5341 - val_loss: 1.1515 - val_acc: 0.5278\n",
            "Epoch 243/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0758 - acc: 0.5382 - val_loss: 1.1506 - val_acc: 0.5333\n",
            "Epoch 244/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0754 - acc: 0.5355 - val_loss: 1.1520 - val_acc: 0.5278\n",
            "Epoch 245/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0743 - acc: 0.5382 - val_loss: 1.1509 - val_acc: 0.5333\n",
            "Epoch 246/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.0737 - acc: 0.5355 - val_loss: 1.1510 - val_acc: 0.5278\n",
            "Epoch 247/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0731 - acc: 0.5355 - val_loss: 1.1508 - val_acc: 0.5333\n",
            "Epoch 248/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0731 - acc: 0.5341 - val_loss: 1.1520 - val_acc: 0.5278\n",
            "Epoch 249/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0723 - acc: 0.5382 - val_loss: 1.1505 - val_acc: 0.5333\n",
            "Epoch 250/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0725 - acc: 0.5355 - val_loss: 1.1512 - val_acc: 0.5278\n",
            "Epoch 251/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0726 - acc: 0.5355 - val_loss: 1.1512 - val_acc: 0.5278\n",
            "Epoch 252/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0713 - acc: 0.5355 - val_loss: 1.1508 - val_acc: 0.5278\n",
            "Epoch 253/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0712 - acc: 0.5355 - val_loss: 1.1522 - val_acc: 0.5278\n",
            "Epoch 254/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0712 - acc: 0.5382 - val_loss: 1.1521 - val_acc: 0.5278\n",
            "Epoch 255/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0701 - acc: 0.5396 - val_loss: 1.1514 - val_acc: 0.5333\n",
            "Epoch 256/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0695 - acc: 0.5355 - val_loss: 1.1505 - val_acc: 0.5333\n",
            "Epoch 257/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0688 - acc: 0.5355 - val_loss: 1.1503 - val_acc: 0.5278\n",
            "Epoch 258/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0687 - acc: 0.5396 - val_loss: 1.1503 - val_acc: 0.5333\n",
            "Epoch 259/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0683 - acc: 0.5382 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 260/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.0670 - acc: 0.5382 - val_loss: 1.1509 - val_acc: 0.5333\n",
            "Epoch 261/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0664 - acc: 0.5369 - val_loss: 1.1530 - val_acc: 0.5056\n",
            "Epoch 262/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0666 - acc: 0.5396 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 263/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0652 - acc: 0.5369 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 264/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0646 - acc: 0.5382 - val_loss: 1.1521 - val_acc: 0.5222\n",
            "Epoch 265/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0644 - acc: 0.5396 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 266/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0644 - acc: 0.5382 - val_loss: 1.1507 - val_acc: 0.5278\n",
            "Epoch 267/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0638 - acc: 0.5410 - val_loss: 1.1502 - val_acc: 0.5278\n",
            "Epoch 268/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0633 - acc: 0.5382 - val_loss: 1.1514 - val_acc: 0.5333\n",
            "Epoch 269/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0623 - acc: 0.5438 - val_loss: 1.1503 - val_acc: 0.5278\n",
            "Epoch 270/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0620 - acc: 0.5396 - val_loss: 1.1502 - val_acc: 0.5278\n",
            "Epoch 271/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.0615 - acc: 0.5382 - val_loss: 1.1504 - val_acc: 0.5278\n",
            "Epoch 272/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0605 - acc: 0.5396 - val_loss: 1.1505 - val_acc: 0.5278\n",
            "Epoch 273/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0600 - acc: 0.5396 - val_loss: 1.1504 - val_acc: 0.5278\n",
            "Epoch 274/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0592 - acc: 0.5424 - val_loss: 1.1508 - val_acc: 0.5278\n",
            "Epoch 275/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0588 - acc: 0.5396 - val_loss: 1.1503 - val_acc: 0.5278\n",
            "Epoch 276/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.0587 - acc: 0.5410 - val_loss: 1.1497 - val_acc: 0.5278\n",
            "Epoch 277/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0575 - acc: 0.5438 - val_loss: 1.1492 - val_acc: 0.5278\n",
            "Epoch 278/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0578 - acc: 0.5396 - val_loss: 1.1507 - val_acc: 0.5278\n",
            "Epoch 279/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0572 - acc: 0.5424 - val_loss: 1.1505 - val_acc: 0.5222\n",
            "Epoch 280/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0555 - acc: 0.5438 - val_loss: 1.1501 - val_acc: 0.5278\n",
            "Epoch 281/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0564 - acc: 0.5410 - val_loss: 1.1498 - val_acc: 0.5278\n",
            "Epoch 282/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0552 - acc: 0.5396 - val_loss: 1.1505 - val_acc: 0.5111\n",
            "Epoch 283/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0549 - acc: 0.5466 - val_loss: 1.1492 - val_acc: 0.5278\n",
            "Epoch 284/1000\n",
            "719/719 [==============================] - 0s 65us/step - loss: 1.0544 - acc: 0.5438 - val_loss: 1.1497 - val_acc: 0.5278\n",
            "Epoch 285/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 1.0533 - acc: 0.5396 - val_loss: 1.1493 - val_acc: 0.5278\n",
            "Epoch 286/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0525 - acc: 0.5424 - val_loss: 1.1502 - val_acc: 0.5111\n",
            "Epoch 287/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0519 - acc: 0.5480 - val_loss: 1.1502 - val_acc: 0.5278\n",
            "Epoch 288/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0516 - acc: 0.5508 - val_loss: 1.1494 - val_acc: 0.5222\n",
            "Epoch 289/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0516 - acc: 0.5410 - val_loss: 1.1498 - val_acc: 0.5111\n",
            "Epoch 290/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0504 - acc: 0.5452 - val_loss: 1.1515 - val_acc: 0.5111\n",
            "Epoch 291/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0507 - acc: 0.5438 - val_loss: 1.1500 - val_acc: 0.5056\n",
            "Epoch 292/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0489 - acc: 0.5480 - val_loss: 1.1484 - val_acc: 0.5222\n",
            "Epoch 293/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0476 - acc: 0.5438 - val_loss: 1.1494 - val_acc: 0.5111\n",
            "Epoch 294/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.0472 - acc: 0.5508 - val_loss: 1.1490 - val_acc: 0.5167\n",
            "Epoch 295/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0465 - acc: 0.5522 - val_loss: 1.1482 - val_acc: 0.5278\n",
            "Epoch 296/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0465 - acc: 0.5452 - val_loss: 1.1484 - val_acc: 0.5278\n",
            "Epoch 297/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0458 - acc: 0.5480 - val_loss: 1.1481 - val_acc: 0.5222\n",
            "Epoch 298/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.0457 - acc: 0.5480 - val_loss: 1.1480 - val_acc: 0.5278\n",
            "Epoch 299/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0443 - acc: 0.5438 - val_loss: 1.1505 - val_acc: 0.5111\n",
            "Epoch 300/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0439 - acc: 0.5508 - val_loss: 1.1475 - val_acc: 0.5222\n",
            "Epoch 301/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0424 - acc: 0.5508 - val_loss: 1.1489 - val_acc: 0.5111\n",
            "Epoch 302/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0422 - acc: 0.5535 - val_loss: 1.1472 - val_acc: 0.5222\n",
            "Epoch 303/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0411 - acc: 0.5480 - val_loss: 1.1465 - val_acc: 0.5222\n",
            "Epoch 304/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0404 - acc: 0.5494 - val_loss: 1.1467 - val_acc: 0.5278\n",
            "Epoch 305/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0408 - acc: 0.5480 - val_loss: 1.1467 - val_acc: 0.5278\n",
            "Epoch 306/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0392 - acc: 0.5466 - val_loss: 1.1475 - val_acc: 0.5111\n",
            "Epoch 307/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0388 - acc: 0.5535 - val_loss: 1.1468 - val_acc: 0.5222\n",
            "Epoch 308/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0379 - acc: 0.5549 - val_loss: 1.1468 - val_acc: 0.5222\n",
            "Epoch 309/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0369 - acc: 0.5549 - val_loss: 1.1457 - val_acc: 0.5167\n",
            "Epoch 310/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 1.0366 - acc: 0.5494 - val_loss: 1.1467 - val_acc: 0.5167\n",
            "Epoch 311/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0350 - acc: 0.5522 - val_loss: 1.1449 - val_acc: 0.5167\n",
            "Epoch 312/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0345 - acc: 0.5508 - val_loss: 1.1448 - val_acc: 0.5222\n",
            "Epoch 313/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0343 - acc: 0.5563 - val_loss: 1.1446 - val_acc: 0.5222\n",
            "Epoch 314/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0332 - acc: 0.5549 - val_loss: 1.1451 - val_acc: 0.5222\n",
            "Epoch 315/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0321 - acc: 0.5563 - val_loss: 1.1440 - val_acc: 0.5222\n",
            "Epoch 316/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0316 - acc: 0.5577 - val_loss: 1.1431 - val_acc: 0.5278\n",
            "Epoch 317/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0308 - acc: 0.5535 - val_loss: 1.1432 - val_acc: 0.5222\n",
            "Epoch 318/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0300 - acc: 0.5549 - val_loss: 1.1439 - val_acc: 0.5167\n",
            "Epoch 319/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0286 - acc: 0.5591 - val_loss: 1.1456 - val_acc: 0.5222\n",
            "Epoch 320/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0284 - acc: 0.5591 - val_loss: 1.1423 - val_acc: 0.5278\n",
            "Epoch 321/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0270 - acc: 0.5591 - val_loss: 1.1424 - val_acc: 0.5222\n",
            "Epoch 322/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0266 - acc: 0.5605 - val_loss: 1.1417 - val_acc: 0.5278\n",
            "Epoch 323/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 1.0256 - acc: 0.5577 - val_loss: 1.1427 - val_acc: 0.5222\n",
            "Epoch 324/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0255 - acc: 0.5535 - val_loss: 1.1414 - val_acc: 0.5222\n",
            "Epoch 325/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0244 - acc: 0.5619 - val_loss: 1.1428 - val_acc: 0.5167\n",
            "Epoch 326/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0234 - acc: 0.5633 - val_loss: 1.1422 - val_acc: 0.5222\n",
            "Epoch 327/1000\n",
            "719/719 [==============================] - 0s 65us/step - loss: 1.0223 - acc: 0.5661 - val_loss: 1.1409 - val_acc: 0.5278\n",
            "Epoch 328/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 1.0212 - acc: 0.5633 - val_loss: 1.1403 - val_acc: 0.5278\n",
            "Epoch 329/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0213 - acc: 0.5619 - val_loss: 1.1402 - val_acc: 0.5222\n",
            "Epoch 330/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0195 - acc: 0.5577 - val_loss: 1.1404 - val_acc: 0.5222\n",
            "Epoch 331/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 1.0186 - acc: 0.5716 - val_loss: 1.1404 - val_acc: 0.5222\n",
            "Epoch 332/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 1.0177 - acc: 0.5661 - val_loss: 1.1398 - val_acc: 0.5222\n",
            "Epoch 333/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0162 - acc: 0.5688 - val_loss: 1.1377 - val_acc: 0.5222\n",
            "Epoch 334/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0148 - acc: 0.5647 - val_loss: 1.1394 - val_acc: 0.5167\n",
            "Epoch 335/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0147 - acc: 0.5688 - val_loss: 1.1358 - val_acc: 0.5222\n",
            "Epoch 336/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0131 - acc: 0.5675 - val_loss: 1.1367 - val_acc: 0.5222\n",
            "Epoch 337/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.0121 - acc: 0.5688 - val_loss: 1.1354 - val_acc: 0.5222\n",
            "Epoch 338/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 1.0109 - acc: 0.5702 - val_loss: 1.1353 - val_acc: 0.5222\n",
            "Epoch 339/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 1.0094 - acc: 0.5702 - val_loss: 1.1333 - val_acc: 0.5222\n",
            "Epoch 340/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.0084 - acc: 0.5647 - val_loss: 1.1333 - val_acc: 0.5333\n",
            "Epoch 341/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 1.0074 - acc: 0.5744 - val_loss: 1.1319 - val_acc: 0.5333\n",
            "Epoch 342/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0065 - acc: 0.5688 - val_loss: 1.1307 - val_acc: 0.5167\n",
            "Epoch 343/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 1.0050 - acc: 0.5702 - val_loss: 1.1295 - val_acc: 0.5278\n",
            "Epoch 344/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 1.0037 - acc: 0.5730 - val_loss: 1.1273 - val_acc: 0.5222\n",
            "Epoch 345/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 1.0024 - acc: 0.5730 - val_loss: 1.1268 - val_acc: 0.5222\n",
            "Epoch 346/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 1.0013 - acc: 0.5702 - val_loss: 1.1263 - val_acc: 0.5278\n",
            "Epoch 347/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9998 - acc: 0.5688 - val_loss: 1.1250 - val_acc: 0.5222\n",
            "Epoch 348/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.9990 - acc: 0.5716 - val_loss: 1.1240 - val_acc: 0.5333\n",
            "Epoch 349/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.9983 - acc: 0.5744 - val_loss: 1.1239 - val_acc: 0.5556\n",
            "Epoch 350/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9958 - acc: 0.5786 - val_loss: 1.1209 - val_acc: 0.5444\n",
            "Epoch 351/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9952 - acc: 0.5772 - val_loss: 1.1202 - val_acc: 0.5444\n",
            "Epoch 352/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9928 - acc: 0.5730 - val_loss: 1.1185 - val_acc: 0.5278\n",
            "Epoch 353/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9919 - acc: 0.5800 - val_loss: 1.1181 - val_acc: 0.5333\n",
            "Epoch 354/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9895 - acc: 0.5772 - val_loss: 1.1186 - val_acc: 0.5556\n",
            "Epoch 355/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.9875 - acc: 0.5953 - val_loss: 1.1146 - val_acc: 0.5333\n",
            "Epoch 356/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9870 - acc: 0.5814 - val_loss: 1.1134 - val_acc: 0.5389\n",
            "Epoch 357/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9852 - acc: 0.5869 - val_loss: 1.1110 - val_acc: 0.5389\n",
            "Epoch 358/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.9838 - acc: 0.5841 - val_loss: 1.1108 - val_acc: 0.5500\n",
            "Epoch 359/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9817 - acc: 0.5925 - val_loss: 1.1099 - val_acc: 0.5389\n",
            "Epoch 360/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9811 - acc: 0.5883 - val_loss: 1.1094 - val_acc: 0.5389\n",
            "Epoch 361/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9807 - acc: 0.5883 - val_loss: 1.1085 - val_acc: 0.5389\n",
            "Epoch 362/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 0.9777 - acc: 0.5911 - val_loss: 1.1090 - val_acc: 0.5611\n",
            "Epoch 363/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9766 - acc: 0.5953 - val_loss: 1.1084 - val_acc: 0.5611\n",
            "Epoch 364/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9756 - acc: 0.6008 - val_loss: 1.1064 - val_acc: 0.5389\n",
            "Epoch 365/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9739 - acc: 0.5953 - val_loss: 1.1063 - val_acc: 0.5611\n",
            "Epoch 366/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9714 - acc: 0.5981 - val_loss: 1.1037 - val_acc: 0.5611\n",
            "Epoch 367/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.9713 - acc: 0.6036 - val_loss: 1.1041 - val_acc: 0.5556\n",
            "Epoch 368/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9685 - acc: 0.6120 - val_loss: 1.1034 - val_acc: 0.5444\n",
            "Epoch 369/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9683 - acc: 0.5994 - val_loss: 1.1026 - val_acc: 0.5722\n",
            "Epoch 370/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 0.9658 - acc: 0.6064 - val_loss: 1.1007 - val_acc: 0.5722\n",
            "Epoch 371/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9657 - acc: 0.6092 - val_loss: 1.0998 - val_acc: 0.5500\n",
            "Epoch 372/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9642 - acc: 0.6217 - val_loss: 1.0983 - val_acc: 0.5444\n",
            "Epoch 373/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.9616 - acc: 0.6050 - val_loss: 1.1045 - val_acc: 0.5667\n",
            "Epoch 374/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9605 - acc: 0.6175 - val_loss: 1.0994 - val_acc: 0.5944\n",
            "Epoch 375/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9580 - acc: 0.6161 - val_loss: 1.0974 - val_acc: 0.6000\n",
            "Epoch 376/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9569 - acc: 0.6189 - val_loss: 1.0947 - val_acc: 0.5833\n",
            "Epoch 377/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9553 - acc: 0.6231 - val_loss: 1.0934 - val_acc: 0.5778\n",
            "Epoch 378/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9539 - acc: 0.6175 - val_loss: 1.0927 - val_acc: 0.5833\n",
            "Epoch 379/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9520 - acc: 0.6189 - val_loss: 1.0918 - val_acc: 0.5722\n",
            "Epoch 380/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9494 - acc: 0.6175 - val_loss: 1.0910 - val_acc: 0.5556\n",
            "Epoch 381/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9497 - acc: 0.6161 - val_loss: 1.0897 - val_acc: 0.6000\n",
            "Epoch 382/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.9468 - acc: 0.6147 - val_loss: 1.0917 - val_acc: 0.5944\n",
            "Epoch 383/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9450 - acc: 0.6259 - val_loss: 1.0890 - val_acc: 0.5944\n",
            "Epoch 384/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9449 - acc: 0.6287 - val_loss: 1.0867 - val_acc: 0.6056\n",
            "Epoch 385/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9413 - acc: 0.6314 - val_loss: 1.0848 - val_acc: 0.5722\n",
            "Epoch 386/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9401 - acc: 0.6328 - val_loss: 1.0822 - val_acc: 0.5722\n",
            "Epoch 387/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.9379 - acc: 0.6245 - val_loss: 1.0844 - val_acc: 0.6000\n",
            "Epoch 388/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9370 - acc: 0.6370 - val_loss: 1.0809 - val_acc: 0.6167\n",
            "Epoch 389/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9343 - acc: 0.6287 - val_loss: 1.0795 - val_acc: 0.5722\n",
            "Epoch 390/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9335 - acc: 0.6217 - val_loss: 1.0792 - val_acc: 0.6056\n",
            "Epoch 391/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.9306 - acc: 0.6342 - val_loss: 1.0766 - val_acc: 0.5778\n",
            "Epoch 392/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9294 - acc: 0.6314 - val_loss: 1.0748 - val_acc: 0.6056\n",
            "Epoch 393/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 0.9269 - acc: 0.6287 - val_loss: 1.0803 - val_acc: 0.5722\n",
            "Epoch 394/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9262 - acc: 0.6439 - val_loss: 1.0710 - val_acc: 0.6056\n",
            "Epoch 395/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9243 - acc: 0.6356 - val_loss: 1.0695 - val_acc: 0.6056\n",
            "Epoch 396/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9214 - acc: 0.6412 - val_loss: 1.0690 - val_acc: 0.6056\n",
            "Epoch 397/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.9206 - acc: 0.6398 - val_loss: 1.0682 - val_acc: 0.6056\n",
            "Epoch 398/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9186 - acc: 0.6370 - val_loss: 1.0674 - val_acc: 0.6056\n",
            "Epoch 399/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.9163 - acc: 0.6398 - val_loss: 1.0638 - val_acc: 0.6111\n",
            "Epoch 400/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9141 - acc: 0.6398 - val_loss: 1.0635 - val_acc: 0.6056\n",
            "Epoch 401/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.9119 - acc: 0.6426 - val_loss: 1.0616 - val_acc: 0.6000\n",
            "Epoch 402/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9109 - acc: 0.6453 - val_loss: 1.0597 - val_acc: 0.6111\n",
            "Epoch 403/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9086 - acc: 0.6439 - val_loss: 1.0585 - val_acc: 0.6111\n",
            "Epoch 404/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.9055 - acc: 0.6370 - val_loss: 1.0616 - val_acc: 0.6000\n",
            "Epoch 405/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.9041 - acc: 0.6509 - val_loss: 1.0604 - val_acc: 0.6000\n",
            "Epoch 406/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.9029 - acc: 0.6495 - val_loss: 1.0545 - val_acc: 0.6111\n",
            "Epoch 407/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.9014 - acc: 0.6453 - val_loss: 1.0541 - val_acc: 0.6000\n",
            "Epoch 408/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8976 - acc: 0.6509 - val_loss: 1.0539 - val_acc: 0.6056\n",
            "Epoch 409/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.8957 - acc: 0.6453 - val_loss: 1.0488 - val_acc: 0.6111\n",
            "Epoch 410/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.8934 - acc: 0.6439 - val_loss: 1.0489 - val_acc: 0.6111\n",
            "Epoch 411/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.8905 - acc: 0.6537 - val_loss: 1.0479 - val_acc: 0.6111\n",
            "Epoch 412/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8892 - acc: 0.6592 - val_loss: 1.0441 - val_acc: 0.6111\n",
            "Epoch 413/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.8867 - acc: 0.6565 - val_loss: 1.0418 - val_acc: 0.6111\n",
            "Epoch 414/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.8849 - acc: 0.6551 - val_loss: 1.0442 - val_acc: 0.6111\n",
            "Epoch 415/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8814 - acc: 0.6579 - val_loss: 1.0381 - val_acc: 0.6167\n",
            "Epoch 416/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.8803 - acc: 0.6606 - val_loss: 1.0371 - val_acc: 0.6111\n",
            "Epoch 417/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8780 - acc: 0.6551 - val_loss: 1.0348 - val_acc: 0.6222\n",
            "Epoch 418/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.8751 - acc: 0.6634 - val_loss: 1.0322 - val_acc: 0.6111\n",
            "Epoch 419/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.8741 - acc: 0.6606 - val_loss: 1.0348 - val_acc: 0.6167\n",
            "Epoch 420/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.8712 - acc: 0.6634 - val_loss: 1.0323 - val_acc: 0.6167\n",
            "Epoch 421/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.8685 - acc: 0.6662 - val_loss: 1.0318 - val_acc: 0.6167\n",
            "Epoch 422/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.8666 - acc: 0.6634 - val_loss: 1.0286 - val_acc: 0.6222\n",
            "Epoch 423/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.8623 - acc: 0.6676 - val_loss: 1.0311 - val_acc: 0.6111\n",
            "Epoch 424/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.8597 - acc: 0.6662 - val_loss: 1.0486 - val_acc: 0.5833\n",
            "Epoch 425/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.8600 - acc: 0.6732 - val_loss: 1.0204 - val_acc: 0.6167\n",
            "Epoch 426/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8575 - acc: 0.6648 - val_loss: 1.0223 - val_acc: 0.6111\n",
            "Epoch 427/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.8553 - acc: 0.6745 - val_loss: 1.0190 - val_acc: 0.6167\n",
            "Epoch 428/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8511 - acc: 0.6773 - val_loss: 1.0131 - val_acc: 0.6222\n",
            "Epoch 429/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8481 - acc: 0.6690 - val_loss: 1.0177 - val_acc: 0.6278\n",
            "Epoch 430/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.8460 - acc: 0.6745 - val_loss: 1.0144 - val_acc: 0.6222\n",
            "Epoch 431/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.8424 - acc: 0.6787 - val_loss: 1.0050 - val_acc: 0.6167\n",
            "Epoch 432/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8412 - acc: 0.6718 - val_loss: 1.0059 - val_acc: 0.6278\n",
            "Epoch 433/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8384 - acc: 0.6801 - val_loss: 1.0007 - val_acc: 0.6333\n",
            "Epoch 434/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.8353 - acc: 0.6759 - val_loss: 1.0005 - val_acc: 0.6278\n",
            "Epoch 435/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8330 - acc: 0.6773 - val_loss: 1.0006 - val_acc: 0.6222\n",
            "Epoch 436/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.8312 - acc: 0.6801 - val_loss: 0.9968 - val_acc: 0.6222\n",
            "Epoch 437/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8266 - acc: 0.6815 - val_loss: 0.9928 - val_acc: 0.6389\n",
            "Epoch 438/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.8244 - acc: 0.6815 - val_loss: 0.9918 - val_acc: 0.6278\n",
            "Epoch 439/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.8225 - acc: 0.6843 - val_loss: 0.9928 - val_acc: 0.6222\n",
            "Epoch 440/1000\n",
            "719/719 [==============================] - 0s 64us/step - loss: 0.8187 - acc: 0.6857 - val_loss: 0.9890 - val_acc: 0.6333\n",
            "Epoch 441/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.8155 - acc: 0.6871 - val_loss: 0.9839 - val_acc: 0.6389\n",
            "Epoch 442/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.8138 - acc: 0.6843 - val_loss: 0.9818 - val_acc: 0.6389\n",
            "Epoch 443/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8107 - acc: 0.6871 - val_loss: 0.9843 - val_acc: 0.6333\n",
            "Epoch 444/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8091 - acc: 0.6898 - val_loss: 0.9763 - val_acc: 0.6278\n",
            "Epoch 445/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.8049 - acc: 0.6926 - val_loss: 0.9743 - val_acc: 0.6444\n",
            "Epoch 446/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.8025 - acc: 0.6885 - val_loss: 0.9748 - val_acc: 0.6333\n",
            "Epoch 447/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.8015 - acc: 0.7051 - val_loss: 0.9675 - val_acc: 0.6278\n",
            "Epoch 448/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7955 - acc: 0.6885 - val_loss: 0.9903 - val_acc: 0.6278\n",
            "Epoch 449/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.7950 - acc: 0.6982 - val_loss: 0.9684 - val_acc: 0.6278\n",
            "Epoch 450/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.7906 - acc: 0.7010 - val_loss: 0.9711 - val_acc: 0.6444\n",
            "Epoch 451/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7873 - acc: 0.7038 - val_loss: 0.9672 - val_acc: 0.6444\n",
            "Epoch 452/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.7855 - acc: 0.6982 - val_loss: 0.9627 - val_acc: 0.6500\n",
            "Epoch 453/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7839 - acc: 0.7024 - val_loss: 0.9637 - val_acc: 0.6278\n",
            "Epoch 454/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.7810 - acc: 0.7051 - val_loss: 0.9556 - val_acc: 0.6389\n",
            "Epoch 455/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7765 - acc: 0.7010 - val_loss: 0.9581 - val_acc: 0.6444\n",
            "Epoch 456/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.7738 - acc: 0.7065 - val_loss: 0.9481 - val_acc: 0.6333\n",
            "Epoch 457/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7715 - acc: 0.7065 - val_loss: 0.9568 - val_acc: 0.6611\n",
            "Epoch 458/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7679 - acc: 0.7121 - val_loss: 0.9432 - val_acc: 0.6389\n",
            "Epoch 459/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7640 - acc: 0.7107 - val_loss: 0.9433 - val_acc: 0.6333\n",
            "Epoch 460/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.7625 - acc: 0.7051 - val_loss: 0.9463 - val_acc: 0.6556\n",
            "Epoch 461/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.7571 - acc: 0.7065 - val_loss: 0.9473 - val_acc: 0.6500\n",
            "Epoch 462/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7575 - acc: 0.7218 - val_loss: 0.9332 - val_acc: 0.6444\n",
            "Epoch 463/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7529 - acc: 0.7121 - val_loss: 0.9421 - val_acc: 0.6722\n",
            "Epoch 464/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.7493 - acc: 0.7191 - val_loss: 0.9290 - val_acc: 0.6611\n",
            "Epoch 465/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.7463 - acc: 0.7149 - val_loss: 0.9326 - val_acc: 0.6667\n",
            "Epoch 466/1000\n",
            "719/719 [==============================] - 0s 63us/step - loss: 0.7433 - acc: 0.7191 - val_loss: 0.9243 - val_acc: 0.6722\n",
            "Epoch 467/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.7406 - acc: 0.7177 - val_loss: 0.9330 - val_acc: 0.6667\n",
            "Epoch 468/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.7370 - acc: 0.7260 - val_loss: 0.9218 - val_acc: 0.6444\n",
            "Epoch 469/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.7340 - acc: 0.7204 - val_loss: 0.9146 - val_acc: 0.6722\n",
            "Epoch 470/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.7307 - acc: 0.7218 - val_loss: 0.9124 - val_acc: 0.6667\n",
            "Epoch 471/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7283 - acc: 0.7260 - val_loss: 0.9223 - val_acc: 0.6611\n",
            "Epoch 472/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.7246 - acc: 0.7302 - val_loss: 0.9117 - val_acc: 0.6611\n",
            "Epoch 473/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.7204 - acc: 0.7218 - val_loss: 0.9101 - val_acc: 0.6722\n",
            "Epoch 474/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.7176 - acc: 0.7316 - val_loss: 0.9110 - val_acc: 0.6667\n",
            "Epoch 475/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.7152 - acc: 0.7288 - val_loss: 0.9053 - val_acc: 0.6778\n",
            "Epoch 476/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.7119 - acc: 0.7385 - val_loss: 0.8974 - val_acc: 0.6667\n",
            "Epoch 477/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.7088 - acc: 0.7371 - val_loss: 0.9000 - val_acc: 0.6722\n",
            "Epoch 478/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.7064 - acc: 0.7344 - val_loss: 0.8938 - val_acc: 0.6833\n",
            "Epoch 479/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.7041 - acc: 0.7441 - val_loss: 0.8888 - val_acc: 0.6722\n",
            "Epoch 480/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6990 - acc: 0.7371 - val_loss: 0.8879 - val_acc: 0.6833\n",
            "Epoch 481/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.6953 - acc: 0.7524 - val_loss: 0.8831 - val_acc: 0.6778\n",
            "Epoch 482/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.6949 - acc: 0.7441 - val_loss: 0.8813 - val_acc: 0.6778\n",
            "Epoch 483/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6901 - acc: 0.7469 - val_loss: 0.8768 - val_acc: 0.6944\n",
            "Epoch 484/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.6891 - acc: 0.7483 - val_loss: 0.8753 - val_acc: 0.6944\n",
            "Epoch 485/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.6829 - acc: 0.7552 - val_loss: 0.8724 - val_acc: 0.6833\n",
            "Epoch 486/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.6823 - acc: 0.7455 - val_loss: 0.8673 - val_acc: 0.6889\n",
            "Epoch 487/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.6774 - acc: 0.7538 - val_loss: 0.8654 - val_acc: 0.6833\n",
            "Epoch 488/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.6754 - acc: 0.7483 - val_loss: 0.8755 - val_acc: 0.6944\n",
            "Epoch 489/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6721 - acc: 0.7719 - val_loss: 0.8591 - val_acc: 0.6889\n",
            "Epoch 490/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6673 - acc: 0.7677 - val_loss: 0.8601 - val_acc: 0.7056\n",
            "Epoch 491/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.6656 - acc: 0.7677 - val_loss: 0.8747 - val_acc: 0.7056\n",
            "Epoch 492/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6604 - acc: 0.7733 - val_loss: 0.8555 - val_acc: 0.7111\n",
            "Epoch 493/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.6589 - acc: 0.7733 - val_loss: 0.8499 - val_acc: 0.7056\n",
            "Epoch 494/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.6545 - acc: 0.7775 - val_loss: 0.8531 - val_acc: 0.7056\n",
            "Epoch 495/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.6524 - acc: 0.7775 - val_loss: 0.8613 - val_acc: 0.7167\n",
            "Epoch 496/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.6484 - acc: 0.7928 - val_loss: 0.8444 - val_acc: 0.7167\n",
            "Epoch 497/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.6448 - acc: 0.7858 - val_loss: 0.8426 - val_acc: 0.6944\n",
            "Epoch 498/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.6416 - acc: 0.7789 - val_loss: 0.8469 - val_acc: 0.7111\n",
            "Epoch 499/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.6381 - acc: 0.7955 - val_loss: 0.8569 - val_acc: 0.7056\n",
            "Epoch 500/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.6349 - acc: 0.7830 - val_loss: 0.8361 - val_acc: 0.7222\n",
            "Epoch 501/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.6335 - acc: 0.7803 - val_loss: 0.8593 - val_acc: 0.7167\n",
            "Epoch 502/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.6320 - acc: 0.7969 - val_loss: 0.8242 - val_acc: 0.7167\n",
            "Epoch 503/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6254 - acc: 0.7955 - val_loss: 0.8371 - val_acc: 0.7167\n",
            "Epoch 504/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.6222 - acc: 0.7997 - val_loss: 0.8285 - val_acc: 0.7333\n",
            "Epoch 505/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.6188 - acc: 0.8081 - val_loss: 0.8395 - val_acc: 0.7222\n",
            "Epoch 506/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6143 - acc: 0.7942 - val_loss: 0.8438 - val_acc: 0.7278\n",
            "Epoch 507/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.6139 - acc: 0.8067 - val_loss: 0.8234 - val_acc: 0.7278\n",
            "Epoch 508/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.6102 - acc: 0.8108 - val_loss: 0.8140 - val_acc: 0.7444\n",
            "Epoch 509/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.6050 - acc: 0.8095 - val_loss: 0.8130 - val_acc: 0.7389\n",
            "Epoch 510/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.6035 - acc: 0.8081 - val_loss: 0.8031 - val_acc: 0.7278\n",
            "Epoch 511/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.5996 - acc: 0.8011 - val_loss: 0.8096 - val_acc: 0.7389\n",
            "Epoch 512/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.5956 - acc: 0.8136 - val_loss: 0.8043 - val_acc: 0.7500\n",
            "Epoch 513/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.5935 - acc: 0.8248 - val_loss: 0.7994 - val_acc: 0.7389\n",
            "Epoch 514/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.5883 - acc: 0.8108 - val_loss: 0.8052 - val_acc: 0.7278\n",
            "Epoch 515/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.5878 - acc: 0.8164 - val_loss: 0.7921 - val_acc: 0.7556\n",
            "Epoch 516/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.5828 - acc: 0.8261 - val_loss: 0.7832 - val_acc: 0.7556\n",
            "Epoch 517/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.5796 - acc: 0.8303 - val_loss: 0.7839 - val_acc: 0.7444\n",
            "Epoch 518/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.5756 - acc: 0.8220 - val_loss: 0.7820 - val_acc: 0.7556\n",
            "Epoch 519/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5736 - acc: 0.8303 - val_loss: 0.7985 - val_acc: 0.7333\n",
            "Epoch 520/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.5698 - acc: 0.8248 - val_loss: 0.7960 - val_acc: 0.7333\n",
            "Epoch 521/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5676 - acc: 0.8387 - val_loss: 0.7732 - val_acc: 0.7500\n",
            "Epoch 522/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.5656 - acc: 0.8206 - val_loss: 0.7846 - val_acc: 0.7444\n",
            "Epoch 523/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.5613 - acc: 0.8275 - val_loss: 0.7869 - val_acc: 0.7556\n",
            "Epoch 524/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.5587 - acc: 0.8387 - val_loss: 0.7734 - val_acc: 0.7611\n",
            "Epoch 525/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.5554 - acc: 0.8317 - val_loss: 0.7667 - val_acc: 0.7556\n",
            "Epoch 526/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.5509 - acc: 0.8275 - val_loss: 0.7689 - val_acc: 0.7500\n",
            "Epoch 527/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.5490 - acc: 0.8345 - val_loss: 0.7655 - val_acc: 0.7444\n",
            "Epoch 528/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.5465 - acc: 0.8428 - val_loss: 0.7733 - val_acc: 0.7444\n",
            "Epoch 529/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5441 - acc: 0.8387 - val_loss: 0.7568 - val_acc: 0.7667\n",
            "Epoch 530/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.5373 - acc: 0.8373 - val_loss: 0.7495 - val_acc: 0.7722\n",
            "Epoch 531/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.5338 - acc: 0.8512 - val_loss: 0.7460 - val_acc: 0.7833\n",
            "Epoch 532/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.5331 - acc: 0.8414 - val_loss: 0.7613 - val_acc: 0.7556\n",
            "Epoch 533/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.5329 - acc: 0.8456 - val_loss: 0.7420 - val_acc: 0.7778\n",
            "Epoch 534/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.5285 - acc: 0.8498 - val_loss: 0.7550 - val_acc: 0.7500\n",
            "Epoch 535/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5260 - acc: 0.8526 - val_loss: 0.7478 - val_acc: 0.7667\n",
            "Epoch 536/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5212 - acc: 0.8414 - val_loss: 0.7621 - val_acc: 0.7389\n",
            "Epoch 537/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.5180 - acc: 0.8498 - val_loss: 0.7421 - val_acc: 0.7611\n",
            "Epoch 538/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.5175 - acc: 0.8581 - val_loss: 0.7409 - val_acc: 0.7611\n",
            "Epoch 539/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5124 - acc: 0.8595 - val_loss: 0.7259 - val_acc: 0.7722\n",
            "Epoch 540/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.5100 - acc: 0.8512 - val_loss: 0.7480 - val_acc: 0.7556\n",
            "Epoch 541/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.5078 - acc: 0.8595 - val_loss: 0.7201 - val_acc: 0.7944\n",
            "Epoch 542/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.5078 - acc: 0.8554 - val_loss: 0.7211 - val_acc: 0.7722\n",
            "Epoch 543/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.5008 - acc: 0.8609 - val_loss: 0.7196 - val_acc: 0.7778\n",
            "Epoch 544/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.4953 - acc: 0.8679 - val_loss: 0.7136 - val_acc: 0.7944\n",
            "Epoch 545/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4950 - acc: 0.8554 - val_loss: 0.7197 - val_acc: 0.7667\n",
            "Epoch 546/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.4930 - acc: 0.8609 - val_loss: 0.7066 - val_acc: 0.7833\n",
            "Epoch 547/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.4912 - acc: 0.8651 - val_loss: 0.7134 - val_acc: 0.7833\n",
            "Epoch 548/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.4880 - acc: 0.8554 - val_loss: 0.7018 - val_acc: 0.7833\n",
            "Epoch 549/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4848 - acc: 0.8526 - val_loss: 0.6978 - val_acc: 0.7889\n",
            "Epoch 550/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.4811 - acc: 0.8567 - val_loss: 0.7072 - val_acc: 0.7833\n",
            "Epoch 551/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.4787 - acc: 0.8693 - val_loss: 0.7054 - val_acc: 0.7667\n",
            "Epoch 552/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.4752 - acc: 0.8707 - val_loss: 0.6991 - val_acc: 0.7833\n",
            "Epoch 553/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.4714 - acc: 0.8651 - val_loss: 0.6989 - val_acc: 0.7722\n",
            "Epoch 554/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.4706 - acc: 0.8693 - val_loss: 0.6877 - val_acc: 0.7889\n",
            "Epoch 555/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4652 - acc: 0.8637 - val_loss: 0.6851 - val_acc: 0.7833\n",
            "Epoch 556/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.4658 - acc: 0.8679 - val_loss: 0.6983 - val_acc: 0.7722\n",
            "Epoch 557/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.4620 - acc: 0.8776 - val_loss: 0.6794 - val_acc: 0.7889\n",
            "Epoch 558/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.4584 - acc: 0.8776 - val_loss: 0.6994 - val_acc: 0.7667\n",
            "Epoch 559/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.4576 - acc: 0.8720 - val_loss: 0.6717 - val_acc: 0.8056\n",
            "Epoch 560/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.4528 - acc: 0.8818 - val_loss: 0.6706 - val_acc: 0.7944\n",
            "Epoch 561/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4535 - acc: 0.8748 - val_loss: 0.6754 - val_acc: 0.7944\n",
            "Epoch 562/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.4486 - acc: 0.8818 - val_loss: 0.6709 - val_acc: 0.8000\n",
            "Epoch 563/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.4460 - acc: 0.8762 - val_loss: 0.6653 - val_acc: 0.7944\n",
            "Epoch 564/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.4452 - acc: 0.8776 - val_loss: 0.6828 - val_acc: 0.7833\n",
            "Epoch 565/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.4414 - acc: 0.8790 - val_loss: 0.6693 - val_acc: 0.8056\n",
            "Epoch 566/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.4361 - acc: 0.8846 - val_loss: 0.6545 - val_acc: 0.8000\n",
            "Epoch 567/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.4366 - acc: 0.8832 - val_loss: 0.6693 - val_acc: 0.8056\n",
            "Epoch 568/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.4338 - acc: 0.8790 - val_loss: 0.6695 - val_acc: 0.7944\n",
            "Epoch 569/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4309 - acc: 0.8832 - val_loss: 0.6531 - val_acc: 0.8056\n",
            "Epoch 570/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.4288 - acc: 0.8846 - val_loss: 0.6684 - val_acc: 0.7889\n",
            "Epoch 571/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.4249 - acc: 0.8873 - val_loss: 0.6535 - val_acc: 0.8000\n",
            "Epoch 572/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.4237 - acc: 0.8915 - val_loss: 0.6513 - val_acc: 0.8000\n",
            "Epoch 573/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4193 - acc: 0.8943 - val_loss: 0.6501 - val_acc: 0.8000\n",
            "Epoch 574/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.4216 - acc: 0.8915 - val_loss: 0.6410 - val_acc: 0.8056\n",
            "Epoch 575/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.4146 - acc: 0.8873 - val_loss: 0.6477 - val_acc: 0.8111\n",
            "Epoch 576/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.4136 - acc: 0.8929 - val_loss: 0.6391 - val_acc: 0.8111\n",
            "Epoch 577/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.4103 - acc: 0.8887 - val_loss: 0.6311 - val_acc: 0.8056\n",
            "Epoch 578/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.4077 - acc: 0.8901 - val_loss: 0.6302 - val_acc: 0.8111\n",
            "Epoch 579/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.4065 - acc: 0.8832 - val_loss: 0.6371 - val_acc: 0.8000\n",
            "Epoch 580/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.4026 - acc: 0.8971 - val_loss: 0.6417 - val_acc: 0.8111\n",
            "Epoch 581/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3993 - acc: 0.8985 - val_loss: 0.6188 - val_acc: 0.8111\n",
            "Epoch 582/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.3990 - acc: 0.8957 - val_loss: 0.6263 - val_acc: 0.8056\n",
            "Epoch 583/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3963 - acc: 0.8887 - val_loss: 0.6173 - val_acc: 0.7944\n",
            "Epoch 584/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.3933 - acc: 0.8957 - val_loss: 0.6286 - val_acc: 0.8056\n",
            "Epoch 585/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.3920 - acc: 0.8971 - val_loss: 0.6183 - val_acc: 0.8167\n",
            "Epoch 586/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.3895 - acc: 0.8929 - val_loss: 0.6210 - val_acc: 0.8167\n",
            "Epoch 587/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3869 - acc: 0.8901 - val_loss: 0.6179 - val_acc: 0.8056\n",
            "Epoch 588/1000\n",
            "719/719 [==============================] - 0s 64us/step - loss: 0.3835 - acc: 0.9068 - val_loss: 0.6283 - val_acc: 0.8111\n",
            "Epoch 589/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3886 - acc: 0.8943 - val_loss: 0.6272 - val_acc: 0.8000\n",
            "Epoch 590/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3802 - acc: 0.8929 - val_loss: 0.6285 - val_acc: 0.8167\n",
            "Epoch 591/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3784 - acc: 0.8957 - val_loss: 0.6128 - val_acc: 0.8167\n",
            "Epoch 592/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.3739 - acc: 0.9040 - val_loss: 0.6062 - val_acc: 0.8222\n",
            "Epoch 593/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3752 - acc: 0.9068 - val_loss: 0.5988 - val_acc: 0.8222\n",
            "Epoch 594/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.3707 - acc: 0.9026 - val_loss: 0.5922 - val_acc: 0.8111\n",
            "Epoch 595/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3694 - acc: 0.8999 - val_loss: 0.5965 - val_acc: 0.8111\n",
            "Epoch 596/1000\n",
            "719/719 [==============================] - 0s 62us/step - loss: 0.3671 - acc: 0.9040 - val_loss: 0.6065 - val_acc: 0.8056\n",
            "Epoch 597/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.3639 - acc: 0.9193 - val_loss: 0.5923 - val_acc: 0.8111\n",
            "Epoch 598/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.3643 - acc: 0.9124 - val_loss: 0.5962 - val_acc: 0.8056\n",
            "Epoch 599/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.3592 - acc: 0.9026 - val_loss: 0.5969 - val_acc: 0.8222\n",
            "Epoch 600/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.3585 - acc: 0.9082 - val_loss: 0.6054 - val_acc: 0.8222\n",
            "Epoch 601/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3595 - acc: 0.9054 - val_loss: 0.5946 - val_acc: 0.8111\n",
            "Epoch 602/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.3536 - acc: 0.9110 - val_loss: 0.5822 - val_acc: 0.8111\n",
            "Epoch 603/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3546 - acc: 0.9138 - val_loss: 0.5804 - val_acc: 0.8111\n",
            "Epoch 604/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.3507 - acc: 0.9138 - val_loss: 0.6073 - val_acc: 0.8222\n",
            "Epoch 605/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3500 - acc: 0.9138 - val_loss: 0.5723 - val_acc: 0.8222\n",
            "Epoch 606/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.3480 - acc: 0.9166 - val_loss: 0.5756 - val_acc: 0.8222\n",
            "Epoch 607/1000\n",
            "719/719 [==============================] - 0s 64us/step - loss: 0.3452 - acc: 0.9193 - val_loss: 0.5814 - val_acc: 0.8111\n",
            "Epoch 608/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.3450 - acc: 0.9096 - val_loss: 0.5967 - val_acc: 0.8278\n",
            "Epoch 609/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.3417 - acc: 0.9166 - val_loss: 0.5739 - val_acc: 0.8222\n",
            "Epoch 610/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.3391 - acc: 0.9179 - val_loss: 0.5701 - val_acc: 0.8278\n",
            "Epoch 611/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.3380 - acc: 0.9110 - val_loss: 0.5805 - val_acc: 0.8222\n",
            "Epoch 612/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.3338 - acc: 0.9096 - val_loss: 0.5784 - val_acc: 0.8222\n",
            "Epoch 613/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3351 - acc: 0.9152 - val_loss: 0.5889 - val_acc: 0.8278\n",
            "Epoch 614/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.3333 - acc: 0.9179 - val_loss: 0.5572 - val_acc: 0.8222\n",
            "Epoch 615/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.3316 - acc: 0.9124 - val_loss: 0.5539 - val_acc: 0.8167\n",
            "Epoch 616/1000\n",
            "719/719 [==============================] - 0s 64us/step - loss: 0.3289 - acc: 0.9207 - val_loss: 0.5667 - val_acc: 0.8167\n",
            "Epoch 617/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.3281 - acc: 0.9110 - val_loss: 0.5597 - val_acc: 0.8278\n",
            "Epoch 618/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.3254 - acc: 0.9152 - val_loss: 0.5565 - val_acc: 0.8167\n",
            "Epoch 619/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.3234 - acc: 0.9166 - val_loss: 0.5491 - val_acc: 0.8278\n",
            "Epoch 620/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3212 - acc: 0.9221 - val_loss: 0.5988 - val_acc: 0.8056\n",
            "Epoch 621/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3217 - acc: 0.9235 - val_loss: 0.5476 - val_acc: 0.8278\n",
            "Epoch 622/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3187 - acc: 0.9193 - val_loss: 0.5484 - val_acc: 0.8278\n",
            "Epoch 623/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.3179 - acc: 0.9166 - val_loss: 0.5589 - val_acc: 0.8333\n",
            "Epoch 624/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3134 - acc: 0.9235 - val_loss: 0.5504 - val_acc: 0.8167\n",
            "Epoch 625/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.3110 - acc: 0.9207 - val_loss: 0.5605 - val_acc: 0.8333\n",
            "Epoch 626/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.3101 - acc: 0.9249 - val_loss: 0.5680 - val_acc: 0.8278\n",
            "Epoch 627/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.3113 - acc: 0.9249 - val_loss: 0.5403 - val_acc: 0.8333\n",
            "Epoch 628/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.3069 - acc: 0.9249 - val_loss: 0.5467 - val_acc: 0.8278\n",
            "Epoch 629/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.3063 - acc: 0.9235 - val_loss: 0.5497 - val_acc: 0.8389\n",
            "Epoch 630/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.3063 - acc: 0.9249 - val_loss: 0.5695 - val_acc: 0.8333\n",
            "Epoch 631/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.3028 - acc: 0.9193 - val_loss: 0.5424 - val_acc: 0.8333\n",
            "Epoch 632/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2961 - acc: 0.9291 - val_loss: 0.5384 - val_acc: 0.8389\n",
            "Epoch 633/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.3049 - acc: 0.9221 - val_loss: 0.5470 - val_acc: 0.8222\n",
            "Epoch 634/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.2969 - acc: 0.9263 - val_loss: 0.5314 - val_acc: 0.8500\n",
            "Epoch 635/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.2946 - acc: 0.9277 - val_loss: 0.5764 - val_acc: 0.8111\n",
            "Epoch 636/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2964 - acc: 0.9305 - val_loss: 0.5252 - val_acc: 0.8556\n",
            "Epoch 637/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.2918 - acc: 0.9291 - val_loss: 0.5259 - val_acc: 0.8500\n",
            "Epoch 638/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.2896 - acc: 0.9318 - val_loss: 0.5368 - val_acc: 0.8278\n",
            "Epoch 639/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2900 - acc: 0.9277 - val_loss: 0.5237 - val_acc: 0.8333\n",
            "Epoch 640/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2896 - acc: 0.9291 - val_loss: 0.5321 - val_acc: 0.8278\n",
            "Epoch 641/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.2878 - acc: 0.9332 - val_loss: 0.5315 - val_acc: 0.8278\n",
            "Epoch 642/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.2847 - acc: 0.9305 - val_loss: 0.5436 - val_acc: 0.8222\n",
            "Epoch 643/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2829 - acc: 0.9332 - val_loss: 0.5169 - val_acc: 0.8389\n",
            "Epoch 644/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2837 - acc: 0.9332 - val_loss: 0.5157 - val_acc: 0.8500\n",
            "Epoch 645/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2825 - acc: 0.9291 - val_loss: 0.5189 - val_acc: 0.8278\n",
            "Epoch 646/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.2802 - acc: 0.9318 - val_loss: 0.5285 - val_acc: 0.8333\n",
            "Epoch 647/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2787 - acc: 0.9318 - val_loss: 0.5194 - val_acc: 0.8389\n",
            "Epoch 648/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2768 - acc: 0.9374 - val_loss: 0.5169 - val_acc: 0.8222\n",
            "Epoch 649/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2768 - acc: 0.9291 - val_loss: 0.5170 - val_acc: 0.8333\n",
            "Epoch 650/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2711 - acc: 0.9374 - val_loss: 0.5102 - val_acc: 0.8333\n",
            "Epoch 651/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2713 - acc: 0.9332 - val_loss: 0.5038 - val_acc: 0.8389\n",
            "Epoch 652/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2748 - acc: 0.9360 - val_loss: 0.5172 - val_acc: 0.8389\n",
            "Epoch 653/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2684 - acc: 0.9402 - val_loss: 0.4964 - val_acc: 0.8389\n",
            "Epoch 654/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2672 - acc: 0.9291 - val_loss: 0.5255 - val_acc: 0.8333\n",
            "Epoch 655/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2676 - acc: 0.9374 - val_loss: 0.5118 - val_acc: 0.8333\n",
            "Epoch 656/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2641 - acc: 0.9332 - val_loss: 0.5127 - val_acc: 0.8278\n",
            "Epoch 657/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2647 - acc: 0.9416 - val_loss: 0.4927 - val_acc: 0.8333\n",
            "Epoch 658/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2618 - acc: 0.9318 - val_loss: 0.5083 - val_acc: 0.8556\n",
            "Epoch 659/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2609 - acc: 0.9402 - val_loss: 0.4957 - val_acc: 0.8222\n",
            "Epoch 660/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2593 - acc: 0.9402 - val_loss: 0.5016 - val_acc: 0.8444\n",
            "Epoch 661/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2603 - acc: 0.9360 - val_loss: 0.4993 - val_acc: 0.8278\n",
            "Epoch 662/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2570 - acc: 0.9388 - val_loss: 0.4867 - val_acc: 0.8444\n",
            "Epoch 663/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2575 - acc: 0.9305 - val_loss: 0.5051 - val_acc: 0.8389\n",
            "Epoch 664/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2516 - acc: 0.9416 - val_loss: 0.4920 - val_acc: 0.8389\n",
            "Epoch 665/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2532 - acc: 0.9388 - val_loss: 0.4913 - val_acc: 0.8333\n",
            "Epoch 666/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2514 - acc: 0.9458 - val_loss: 0.5063 - val_acc: 0.8444\n",
            "Epoch 667/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2506 - acc: 0.9388 - val_loss: 0.4885 - val_acc: 0.8389\n",
            "Epoch 668/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2476 - acc: 0.9402 - val_loss: 0.4850 - val_acc: 0.8333\n",
            "Epoch 669/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2503 - acc: 0.9318 - val_loss: 0.5575 - val_acc: 0.8222\n",
            "Epoch 670/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2497 - acc: 0.9471 - val_loss: 0.4795 - val_acc: 0.8389\n",
            "Epoch 671/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2449 - acc: 0.9388 - val_loss: 0.4810 - val_acc: 0.8500\n",
            "Epoch 672/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.2444 - acc: 0.9374 - val_loss: 0.5203 - val_acc: 0.8278\n",
            "Epoch 673/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.2434 - acc: 0.9416 - val_loss: 0.5112 - val_acc: 0.8333\n",
            "Epoch 674/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2432 - acc: 0.9416 - val_loss: 0.5210 - val_acc: 0.8444\n",
            "Epoch 675/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.2398 - acc: 0.9374 - val_loss: 0.5009 - val_acc: 0.8389\n",
            "Epoch 676/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2377 - acc: 0.9416 - val_loss: 0.4846 - val_acc: 0.8444\n",
            "Epoch 677/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2356 - acc: 0.9444 - val_loss: 0.4836 - val_acc: 0.8500\n",
            "Epoch 678/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.2354 - acc: 0.9388 - val_loss: 0.4738 - val_acc: 0.8500\n",
            "Epoch 679/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.2339 - acc: 0.9402 - val_loss: 0.4757 - val_acc: 0.8389\n",
            "Epoch 680/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.2335 - acc: 0.9471 - val_loss: 0.4765 - val_acc: 0.8444\n",
            "Epoch 681/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2328 - acc: 0.9402 - val_loss: 0.4718 - val_acc: 0.8500\n",
            "Epoch 682/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2323 - acc: 0.9402 - val_loss: 0.4679 - val_acc: 0.8333\n",
            "Epoch 683/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.2299 - acc: 0.9430 - val_loss: 0.4919 - val_acc: 0.8444\n",
            "Epoch 684/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.2315 - acc: 0.9458 - val_loss: 0.4743 - val_acc: 0.8333\n",
            "Epoch 685/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2293 - acc: 0.9402 - val_loss: 0.4981 - val_acc: 0.8278\n",
            "Epoch 686/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2277 - acc: 0.9430 - val_loss: 0.4886 - val_acc: 0.8389\n",
            "Epoch 687/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2253 - acc: 0.9485 - val_loss: 0.5242 - val_acc: 0.8389\n",
            "Epoch 688/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2274 - acc: 0.9444 - val_loss: 0.4660 - val_acc: 0.8389\n",
            "Epoch 689/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2224 - acc: 0.9485 - val_loss: 0.4709 - val_acc: 0.8389\n",
            "Epoch 690/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2227 - acc: 0.9416 - val_loss: 0.4664 - val_acc: 0.8389\n",
            "Epoch 691/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2209 - acc: 0.9471 - val_loss: 0.4697 - val_acc: 0.8500\n",
            "Epoch 692/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2237 - acc: 0.9402 - val_loss: 0.4589 - val_acc: 0.8389\n",
            "Epoch 693/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2217 - acc: 0.9416 - val_loss: 0.4800 - val_acc: 0.8389\n",
            "Epoch 694/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.2199 - acc: 0.9416 - val_loss: 0.4752 - val_acc: 0.8389\n",
            "Epoch 695/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.2202 - acc: 0.9485 - val_loss: 0.4723 - val_acc: 0.8500\n",
            "Epoch 696/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2151 - acc: 0.9471 - val_loss: 0.4673 - val_acc: 0.8500\n",
            "Epoch 697/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2165 - acc: 0.9444 - val_loss: 0.4662 - val_acc: 0.8444\n",
            "Epoch 698/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2116 - acc: 0.9499 - val_loss: 0.4465 - val_acc: 0.8556\n",
            "Epoch 699/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2138 - acc: 0.9471 - val_loss: 0.4648 - val_acc: 0.8389\n",
            "Epoch 700/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2132 - acc: 0.9471 - val_loss: 0.4839 - val_acc: 0.8389\n",
            "Epoch 701/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2112 - acc: 0.9513 - val_loss: 0.4677 - val_acc: 0.8389\n",
            "Epoch 702/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.2076 - acc: 0.9471 - val_loss: 0.4674 - val_acc: 0.8500\n",
            "Epoch 703/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2105 - acc: 0.9485 - val_loss: 0.4488 - val_acc: 0.8556\n",
            "Epoch 704/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2066 - acc: 0.9527 - val_loss: 0.4526 - val_acc: 0.8556\n",
            "Epoch 705/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2065 - acc: 0.9444 - val_loss: 0.4633 - val_acc: 0.8389\n",
            "Epoch 706/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.2062 - acc: 0.9485 - val_loss: 0.4542 - val_acc: 0.8500\n",
            "Epoch 707/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.2056 - acc: 0.9513 - val_loss: 0.4537 - val_acc: 0.8556\n",
            "Epoch 708/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2052 - acc: 0.9388 - val_loss: 0.4471 - val_acc: 0.8444\n",
            "Epoch 709/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2035 - acc: 0.9485 - val_loss: 0.4438 - val_acc: 0.8556\n",
            "Epoch 710/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.2018 - acc: 0.9499 - val_loss: 0.4618 - val_acc: 0.8444\n",
            "Epoch 711/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2011 - acc: 0.9555 - val_loss: 0.4528 - val_acc: 0.8389\n",
            "Epoch 712/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.2013 - acc: 0.9583 - val_loss: 0.4470 - val_acc: 0.8556\n",
            "Epoch 713/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.2019 - acc: 0.9485 - val_loss: 0.4488 - val_acc: 0.8556\n",
            "Epoch 714/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1980 - acc: 0.9513 - val_loss: 0.4981 - val_acc: 0.8444\n",
            "Epoch 715/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1979 - acc: 0.9513 - val_loss: 0.4462 - val_acc: 0.8444\n",
            "Epoch 716/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1945 - acc: 0.9541 - val_loss: 0.4561 - val_acc: 0.8611\n",
            "Epoch 717/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1957 - acc: 0.9513 - val_loss: 0.4490 - val_acc: 0.8444\n",
            "Epoch 718/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1936 - acc: 0.9527 - val_loss: 0.4577 - val_acc: 0.8389\n",
            "Epoch 719/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1936 - acc: 0.9555 - val_loss: 0.4482 - val_acc: 0.8611\n",
            "Epoch 720/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1922 - acc: 0.9555 - val_loss: 0.4550 - val_acc: 0.8556\n",
            "Epoch 721/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.1908 - acc: 0.9541 - val_loss: 0.4397 - val_acc: 0.8556\n",
            "Epoch 722/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.1920 - acc: 0.9541 - val_loss: 0.4284 - val_acc: 0.8667\n",
            "Epoch 723/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1880 - acc: 0.9555 - val_loss: 0.4280 - val_acc: 0.8722\n",
            "Epoch 724/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1897 - acc: 0.9555 - val_loss: 0.4616 - val_acc: 0.8444\n",
            "Epoch 725/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1863 - acc: 0.9527 - val_loss: 0.4400 - val_acc: 0.8611\n",
            "Epoch 726/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1855 - acc: 0.9541 - val_loss: 0.4709 - val_acc: 0.8722\n",
            "Epoch 727/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1897 - acc: 0.9471 - val_loss: 0.4410 - val_acc: 0.8500\n",
            "Epoch 728/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1847 - acc: 0.9541 - val_loss: 0.4439 - val_acc: 0.8444\n",
            "Epoch 729/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1875 - acc: 0.9555 - val_loss: 0.4515 - val_acc: 0.8500\n",
            "Epoch 730/1000\n",
            "719/719 [==============================] - 0s 45us/step - loss: 0.1836 - acc: 0.9527 - val_loss: 0.4567 - val_acc: 0.8500\n",
            "Epoch 731/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.1786 - acc: 0.9555 - val_loss: 0.4521 - val_acc: 0.8500\n",
            "Epoch 732/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1786 - acc: 0.9541 - val_loss: 0.4656 - val_acc: 0.8722\n",
            "Epoch 733/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.1819 - acc: 0.9611 - val_loss: 0.4352 - val_acc: 0.8667\n",
            "Epoch 734/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1774 - acc: 0.9624 - val_loss: 0.4315 - val_acc: 0.8667\n",
            "Epoch 735/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1812 - acc: 0.9555 - val_loss: 0.4389 - val_acc: 0.8500\n",
            "Epoch 736/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1759 - acc: 0.9611 - val_loss: 0.4702 - val_acc: 0.8500\n",
            "Epoch 737/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1771 - acc: 0.9638 - val_loss: 0.4248 - val_acc: 0.8611\n",
            "Epoch 738/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1743 - acc: 0.9597 - val_loss: 0.4394 - val_acc: 0.8778\n",
            "Epoch 739/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1730 - acc: 0.9638 - val_loss: 0.4245 - val_acc: 0.8667\n",
            "Epoch 740/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1771 - acc: 0.9555 - val_loss: 0.4466 - val_acc: 0.8611\n",
            "Epoch 741/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1710 - acc: 0.9611 - val_loss: 0.4275 - val_acc: 0.8611\n",
            "Epoch 742/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.1731 - acc: 0.9611 - val_loss: 0.4556 - val_acc: 0.8500\n",
            "Epoch 743/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1721 - acc: 0.9624 - val_loss: 0.4322 - val_acc: 0.8500\n",
            "Epoch 744/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1696 - acc: 0.9624 - val_loss: 0.4464 - val_acc: 0.8444\n",
            "Epoch 745/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1683 - acc: 0.9541 - val_loss: 0.4238 - val_acc: 0.8667\n",
            "Epoch 746/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1658 - acc: 0.9652 - val_loss: 0.4395 - val_acc: 0.8500\n",
            "Epoch 747/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1660 - acc: 0.9652 - val_loss: 0.4144 - val_acc: 0.8722\n",
            "Epoch 748/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1664 - acc: 0.9652 - val_loss: 0.4343 - val_acc: 0.8556\n",
            "Epoch 749/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1676 - acc: 0.9652 - val_loss: 0.4260 - val_acc: 0.8611\n",
            "Epoch 750/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1637 - acc: 0.9652 - val_loss: 0.4347 - val_acc: 0.8611\n",
            "Epoch 751/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1645 - acc: 0.9666 - val_loss: 0.4154 - val_acc: 0.8667\n",
            "Epoch 752/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1636 - acc: 0.9624 - val_loss: 0.4345 - val_acc: 0.8611\n",
            "Epoch 753/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1610 - acc: 0.9708 - val_loss: 0.4379 - val_acc: 0.8556\n",
            "Epoch 754/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1621 - acc: 0.9611 - val_loss: 0.4223 - val_acc: 0.8722\n",
            "Epoch 755/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1579 - acc: 0.9624 - val_loss: 0.4162 - val_acc: 0.8611\n",
            "Epoch 756/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1598 - acc: 0.9652 - val_loss: 0.4465 - val_acc: 0.8444\n",
            "Epoch 757/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1595 - acc: 0.9694 - val_loss: 0.4138 - val_acc: 0.8722\n",
            "Epoch 758/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1576 - acc: 0.9694 - val_loss: 0.4364 - val_acc: 0.8500\n",
            "Epoch 759/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1584 - acc: 0.9611 - val_loss: 0.4205 - val_acc: 0.8500\n",
            "Epoch 760/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1570 - acc: 0.9694 - val_loss: 0.4118 - val_acc: 0.8611\n",
            "Epoch 761/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1585 - acc: 0.9680 - val_loss: 0.4114 - val_acc: 0.8556\n",
            "Epoch 762/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1561 - acc: 0.9694 - val_loss: 0.4095 - val_acc: 0.8611\n",
            "Epoch 763/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1540 - acc: 0.9694 - val_loss: 0.4255 - val_acc: 0.8667\n",
            "Epoch 764/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1550 - acc: 0.9680 - val_loss: 0.4183 - val_acc: 0.8667\n",
            "Epoch 765/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1529 - acc: 0.9708 - val_loss: 0.4211 - val_acc: 0.8556\n",
            "Epoch 766/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.1492 - acc: 0.9708 - val_loss: 0.4312 - val_acc: 0.8611\n",
            "Epoch 767/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1480 - acc: 0.9680 - val_loss: 0.4125 - val_acc: 0.8556\n",
            "Epoch 768/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1498 - acc: 0.9666 - val_loss: 0.4099 - val_acc: 0.8556\n",
            "Epoch 769/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.1501 - acc: 0.9666 - val_loss: 0.4186 - val_acc: 0.8667\n",
            "Epoch 770/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1477 - acc: 0.9708 - val_loss: 0.4138 - val_acc: 0.8722\n",
            "Epoch 771/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1461 - acc: 0.9722 - val_loss: 0.4134 - val_acc: 0.8556\n",
            "Epoch 772/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1489 - acc: 0.9722 - val_loss: 0.4012 - val_acc: 0.8778\n",
            "Epoch 773/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1455 - acc: 0.9694 - val_loss: 0.4165 - val_acc: 0.8667\n",
            "Epoch 774/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1461 - acc: 0.9694 - val_loss: 0.4190 - val_acc: 0.8556\n",
            "Epoch 775/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1468 - acc: 0.9666 - val_loss: 0.4068 - val_acc: 0.8722\n",
            "Epoch 776/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1454 - acc: 0.9694 - val_loss: 0.4105 - val_acc: 0.8556\n",
            "Epoch 777/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1438 - acc: 0.9722 - val_loss: 0.4185 - val_acc: 0.8667\n",
            "Epoch 778/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1419 - acc: 0.9750 - val_loss: 0.4162 - val_acc: 0.8500\n",
            "Epoch 779/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.1422 - acc: 0.9694 - val_loss: 0.4033 - val_acc: 0.8611\n",
            "Epoch 780/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1418 - acc: 0.9736 - val_loss: 0.4196 - val_acc: 0.8500\n",
            "Epoch 781/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1428 - acc: 0.9708 - val_loss: 0.4079 - val_acc: 0.8667\n",
            "Epoch 782/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1415 - acc: 0.9722 - val_loss: 0.4067 - val_acc: 0.8722\n",
            "Epoch 783/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1384 - acc: 0.9750 - val_loss: 0.4041 - val_acc: 0.8667\n",
            "Epoch 784/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1372 - acc: 0.9777 - val_loss: 0.4019 - val_acc: 0.8667\n",
            "Epoch 785/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1374 - acc: 0.9694 - val_loss: 0.4131 - val_acc: 0.8722\n",
            "Epoch 786/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.1411 - acc: 0.9694 - val_loss: 0.4032 - val_acc: 0.8556\n",
            "Epoch 787/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1386 - acc: 0.9764 - val_loss: 0.4010 - val_acc: 0.8667\n",
            "Epoch 788/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1390 - acc: 0.9680 - val_loss: 0.4178 - val_acc: 0.8556\n",
            "Epoch 789/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1344 - acc: 0.9736 - val_loss: 0.4419 - val_acc: 0.8556\n",
            "Epoch 790/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.1369 - acc: 0.9750 - val_loss: 0.4513 - val_acc: 0.8667\n",
            "Epoch 791/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1356 - acc: 0.9764 - val_loss: 0.4200 - val_acc: 0.8556\n",
            "Epoch 792/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1338 - acc: 0.9750 - val_loss: 0.4097 - val_acc: 0.8500\n",
            "Epoch 793/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.1306 - acc: 0.9694 - val_loss: 0.4138 - val_acc: 0.8778\n",
            "Epoch 794/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1308 - acc: 0.9819 - val_loss: 0.4063 - val_acc: 0.8611\n",
            "Epoch 795/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.1307 - acc: 0.9791 - val_loss: 0.4071 - val_acc: 0.8667\n",
            "Epoch 796/1000\n",
            "719/719 [==============================] - 0s 61us/step - loss: 0.1294 - acc: 0.9819 - val_loss: 0.4006 - val_acc: 0.8556\n",
            "Epoch 797/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1315 - acc: 0.9736 - val_loss: 0.4190 - val_acc: 0.8667\n",
            "Epoch 798/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1297 - acc: 0.9764 - val_loss: 0.4091 - val_acc: 0.8556\n",
            "Epoch 799/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.1300 - acc: 0.9736 - val_loss: 0.4090 - val_acc: 0.8667\n",
            "Epoch 800/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1286 - acc: 0.9764 - val_loss: 0.4078 - val_acc: 0.8556\n",
            "Epoch 801/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1261 - acc: 0.9777 - val_loss: 0.4144 - val_acc: 0.8500\n",
            "Epoch 802/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1279 - acc: 0.9764 - val_loss: 0.4082 - val_acc: 0.8611\n",
            "Epoch 803/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1256 - acc: 0.9819 - val_loss: 0.4019 - val_acc: 0.8667\n",
            "Epoch 804/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1281 - acc: 0.9791 - val_loss: 0.3991 - val_acc: 0.8500\n",
            "Epoch 805/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1251 - acc: 0.9833 - val_loss: 0.4039 - val_acc: 0.8500\n",
            "Epoch 806/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1239 - acc: 0.9777 - val_loss: 0.3945 - val_acc: 0.8611\n",
            "Epoch 807/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1236 - acc: 0.9805 - val_loss: 0.4115 - val_acc: 0.8611\n",
            "Epoch 808/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1254 - acc: 0.9764 - val_loss: 0.4074 - val_acc: 0.8611\n",
            "Epoch 809/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.1221 - acc: 0.9791 - val_loss: 0.4061 - val_acc: 0.8611\n",
            "Epoch 810/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1221 - acc: 0.9833 - val_loss: 0.4078 - val_acc: 0.8722\n",
            "Epoch 811/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1218 - acc: 0.9805 - val_loss: 0.3934 - val_acc: 0.8722\n",
            "Epoch 812/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1220 - acc: 0.9764 - val_loss: 0.3924 - val_acc: 0.8667\n",
            "Epoch 813/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1193 - acc: 0.9819 - val_loss: 0.4083 - val_acc: 0.8556\n",
            "Epoch 814/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1197 - acc: 0.9805 - val_loss: 0.3926 - val_acc: 0.8556\n",
            "Epoch 815/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1194 - acc: 0.9805 - val_loss: 0.3864 - val_acc: 0.8722\n",
            "Epoch 816/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1184 - acc: 0.9805 - val_loss: 0.3983 - val_acc: 0.8667\n",
            "Epoch 817/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1178 - acc: 0.9791 - val_loss: 0.3912 - val_acc: 0.8722\n",
            "Epoch 818/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1178 - acc: 0.9819 - val_loss: 0.3902 - val_acc: 0.8667\n",
            "Epoch 819/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1160 - acc: 0.9833 - val_loss: 0.3968 - val_acc: 0.8611\n",
            "Epoch 820/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1172 - acc: 0.9833 - val_loss: 0.3954 - val_acc: 0.8667\n",
            "Epoch 821/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1168 - acc: 0.9833 - val_loss: 0.3939 - val_acc: 0.8667\n",
            "Epoch 822/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.1167 - acc: 0.9777 - val_loss: 0.3894 - val_acc: 0.8667\n",
            "Epoch 823/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1156 - acc: 0.9833 - val_loss: 0.3931 - val_acc: 0.8667\n",
            "Epoch 824/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.1131 - acc: 0.9833 - val_loss: 0.4106 - val_acc: 0.8556\n",
            "Epoch 825/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1148 - acc: 0.9805 - val_loss: 0.3885 - val_acc: 0.8722\n",
            "Epoch 826/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1124 - acc: 0.9791 - val_loss: 0.3911 - val_acc: 0.8722\n",
            "Epoch 827/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.1111 - acc: 0.9833 - val_loss: 0.4095 - val_acc: 0.8722\n",
            "Epoch 828/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.1111 - acc: 0.9833 - val_loss: 0.3868 - val_acc: 0.8722\n",
            "Epoch 829/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1112 - acc: 0.9819 - val_loss: 0.4064 - val_acc: 0.8611\n",
            "Epoch 830/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1106 - acc: 0.9861 - val_loss: 0.4141 - val_acc: 0.8611\n",
            "Epoch 831/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1119 - acc: 0.9819 - val_loss: 0.3997 - val_acc: 0.8611\n",
            "Epoch 832/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1078 - acc: 0.9791 - val_loss: 0.3965 - val_acc: 0.8667\n",
            "Epoch 833/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1120 - acc: 0.9819 - val_loss: 0.3905 - val_acc: 0.8722\n",
            "Epoch 834/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1079 - acc: 0.9861 - val_loss: 0.3957 - val_acc: 0.8722\n",
            "Epoch 835/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1078 - acc: 0.9861 - val_loss: 0.3952 - val_acc: 0.8611\n",
            "Epoch 836/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1059 - acc: 0.9861 - val_loss: 0.3929 - val_acc: 0.8667\n",
            "Epoch 837/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.1068 - acc: 0.9819 - val_loss: 0.3957 - val_acc: 0.8667\n",
            "Epoch 838/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1057 - acc: 0.9861 - val_loss: 0.3974 - val_acc: 0.8611\n",
            "Epoch 839/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1063 - acc: 0.9847 - val_loss: 0.3897 - val_acc: 0.8722\n",
            "Epoch 840/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1058 - acc: 0.9847 - val_loss: 0.4469 - val_acc: 0.8556\n",
            "Epoch 841/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1071 - acc: 0.9819 - val_loss: 0.4249 - val_acc: 0.8722\n",
            "Epoch 842/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.1036 - acc: 0.9847 - val_loss: 0.3803 - val_acc: 0.8667\n",
            "Epoch 843/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1059 - acc: 0.9847 - val_loss: 0.3793 - val_acc: 0.8778\n",
            "Epoch 844/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1035 - acc: 0.9833 - val_loss: 0.4013 - val_acc: 0.8611\n",
            "Epoch 845/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.1025 - acc: 0.9847 - val_loss: 0.3893 - val_acc: 0.8722\n",
            "Epoch 846/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.1019 - acc: 0.9875 - val_loss: 0.3794 - val_acc: 0.8722\n",
            "Epoch 847/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.1019 - acc: 0.9833 - val_loss: 0.3785 - val_acc: 0.8667\n",
            "Epoch 848/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.1038 - acc: 0.9847 - val_loss: 0.4003 - val_acc: 0.8722\n",
            "Epoch 849/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.1016 - acc: 0.9833 - val_loss: 0.4018 - val_acc: 0.8722\n",
            "Epoch 850/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1023 - acc: 0.9819 - val_loss: 0.3946 - val_acc: 0.8611\n",
            "Epoch 851/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.1007 - acc: 0.9861 - val_loss: 0.4057 - val_acc: 0.8611\n",
            "Epoch 852/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0976 - acc: 0.9889 - val_loss: 0.3828 - val_acc: 0.8778\n",
            "Epoch 853/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0999 - acc: 0.9819 - val_loss: 0.3878 - val_acc: 0.8722\n",
            "Epoch 854/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0985 - acc: 0.9847 - val_loss: 0.3861 - val_acc: 0.8722\n",
            "Epoch 855/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0990 - acc: 0.9861 - val_loss: 0.3795 - val_acc: 0.8667\n",
            "Epoch 856/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0966 - acc: 0.9917 - val_loss: 0.3790 - val_acc: 0.8667\n",
            "Epoch 857/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0979 - acc: 0.9875 - val_loss: 0.3774 - val_acc: 0.8722\n",
            "Epoch 858/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0983 - acc: 0.9861 - val_loss: 0.3907 - val_acc: 0.8611\n",
            "Epoch 859/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0981 - acc: 0.9889 - val_loss: 0.3825 - val_acc: 0.8722\n",
            "Epoch 860/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0950 - acc: 0.9875 - val_loss: 0.4039 - val_acc: 0.8722\n",
            "Epoch 861/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0953 - acc: 0.9889 - val_loss: 0.3911 - val_acc: 0.8722\n",
            "Epoch 862/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0942 - acc: 0.9903 - val_loss: 0.3836 - val_acc: 0.8722\n",
            "Epoch 863/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0942 - acc: 0.9861 - val_loss: 0.3863 - val_acc: 0.8722\n",
            "Epoch 864/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.0935 - acc: 0.9903 - val_loss: 0.3804 - val_acc: 0.8722\n",
            "Epoch 865/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0933 - acc: 0.9875 - val_loss: 0.3811 - val_acc: 0.8722\n",
            "Epoch 866/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0932 - acc: 0.9903 - val_loss: 0.3909 - val_acc: 0.8833\n",
            "Epoch 867/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0931 - acc: 0.9875 - val_loss: 0.3742 - val_acc: 0.8722\n",
            "Epoch 868/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0925 - acc: 0.9889 - val_loss: 0.3892 - val_acc: 0.8611\n",
            "Epoch 869/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0907 - acc: 0.9903 - val_loss: 0.3752 - val_acc: 0.8833\n",
            "Epoch 870/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0919 - acc: 0.9889 - val_loss: 0.3802 - val_acc: 0.8722\n",
            "Epoch 871/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0901 - acc: 0.9917 - val_loss: 0.3791 - val_acc: 0.8667\n",
            "Epoch 872/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0912 - acc: 0.9889 - val_loss: 0.3803 - val_acc: 0.8722\n",
            "Epoch 873/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0908 - acc: 0.9889 - val_loss: 0.3836 - val_acc: 0.8833\n",
            "Epoch 874/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0892 - acc: 0.9889 - val_loss: 0.3888 - val_acc: 0.8611\n",
            "Epoch 875/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0887 - acc: 0.9889 - val_loss: 0.3893 - val_acc: 0.8722\n",
            "Epoch 876/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.0895 - acc: 0.9889 - val_loss: 0.3767 - val_acc: 0.8778\n",
            "Epoch 877/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0887 - acc: 0.9889 - val_loss: 0.3796 - val_acc: 0.8667\n",
            "Epoch 878/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0864 - acc: 0.9930 - val_loss: 0.3985 - val_acc: 0.8667\n",
            "Epoch 879/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0883 - acc: 0.9889 - val_loss: 0.3797 - val_acc: 0.8889\n",
            "Epoch 880/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0868 - acc: 0.9903 - val_loss: 0.3769 - val_acc: 0.8667\n",
            "Epoch 881/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.0869 - acc: 0.9903 - val_loss: 0.3812 - val_acc: 0.8667\n",
            "Epoch 882/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0865 - acc: 0.9930 - val_loss: 0.3752 - val_acc: 0.8778\n",
            "Epoch 883/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0853 - acc: 0.9903 - val_loss: 0.3715 - val_acc: 0.8722\n",
            "Epoch 884/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0851 - acc: 0.9944 - val_loss: 0.3700 - val_acc: 0.8722\n",
            "Epoch 885/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0873 - acc: 0.9917 - val_loss: 0.3964 - val_acc: 0.8667\n",
            "Epoch 886/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0851 - acc: 0.9930 - val_loss: 0.3840 - val_acc: 0.8667\n",
            "Epoch 887/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0834 - acc: 0.9917 - val_loss: 0.3861 - val_acc: 0.8778\n",
            "Epoch 888/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0833 - acc: 0.9917 - val_loss: 0.3699 - val_acc: 0.8722\n",
            "Epoch 889/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0848 - acc: 0.9903 - val_loss: 0.3995 - val_acc: 0.8667\n",
            "Epoch 890/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0831 - acc: 0.9903 - val_loss: 0.3790 - val_acc: 0.8667\n",
            "Epoch 891/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0829 - acc: 0.9917 - val_loss: 0.3833 - val_acc: 0.8611\n",
            "Epoch 892/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0820 - acc: 0.9930 - val_loss: 0.3720 - val_acc: 0.8667\n",
            "Epoch 893/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0812 - acc: 0.9917 - val_loss: 0.3935 - val_acc: 0.8667\n",
            "Epoch 894/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0818 - acc: 0.9889 - val_loss: 0.3826 - val_acc: 0.8722\n",
            "Epoch 895/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0816 - acc: 0.9930 - val_loss: 0.3922 - val_acc: 0.8611\n",
            "Epoch 896/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0800 - acc: 0.9958 - val_loss: 0.3795 - val_acc: 0.8722\n",
            "Epoch 897/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0808 - acc: 0.9903 - val_loss: 0.3888 - val_acc: 0.8611\n",
            "Epoch 898/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.0799 - acc: 0.9903 - val_loss: 0.3795 - val_acc: 0.8778\n",
            "Epoch 899/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.0785 - acc: 0.9958 - val_loss: 0.3712 - val_acc: 0.8778\n",
            "Epoch 900/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0805 - acc: 0.9917 - val_loss: 0.3775 - val_acc: 0.8722\n",
            "Epoch 901/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0792 - acc: 0.9958 - val_loss: 0.3787 - val_acc: 0.8611\n",
            "Epoch 902/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0784 - acc: 0.9917 - val_loss: 0.3852 - val_acc: 0.8778\n",
            "Epoch 903/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0780 - acc: 0.9944 - val_loss: 0.3713 - val_acc: 0.8722\n",
            "Epoch 904/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0777 - acc: 0.9930 - val_loss: 0.3963 - val_acc: 0.8722\n",
            "Epoch 905/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0779 - acc: 0.9917 - val_loss: 0.3916 - val_acc: 0.8722\n",
            "Epoch 906/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.0780 - acc: 0.9944 - val_loss: 0.3787 - val_acc: 0.8944\n",
            "Epoch 907/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0774 - acc: 0.9917 - val_loss: 0.3795 - val_acc: 0.8833\n",
            "Epoch 908/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0760 - acc: 0.9917 - val_loss: 0.3674 - val_acc: 0.8722\n",
            "Epoch 909/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0769 - acc: 0.9917 - val_loss: 0.3658 - val_acc: 0.8778\n",
            "Epoch 910/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0751 - acc: 0.9958 - val_loss: 0.3815 - val_acc: 0.8667\n",
            "Epoch 911/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0754 - acc: 0.9944 - val_loss: 0.3856 - val_acc: 0.8722\n",
            "Epoch 912/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0746 - acc: 0.9930 - val_loss: 0.4022 - val_acc: 0.8778\n",
            "Epoch 913/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0754 - acc: 0.9930 - val_loss: 0.3755 - val_acc: 0.8722\n",
            "Epoch 914/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0747 - acc: 0.9930 - val_loss: 0.3857 - val_acc: 0.8722\n",
            "Epoch 915/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0739 - acc: 0.9944 - val_loss: 0.3761 - val_acc: 0.8778\n",
            "Epoch 916/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0748 - acc: 0.9944 - val_loss: 0.3763 - val_acc: 0.8722\n",
            "Epoch 917/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0729 - acc: 0.9930 - val_loss: 0.3765 - val_acc: 0.8833\n",
            "Epoch 918/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0721 - acc: 0.9958 - val_loss: 0.3781 - val_acc: 0.8889\n",
            "Epoch 919/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0749 - acc: 0.9903 - val_loss: 0.3677 - val_acc: 0.8778\n",
            "Epoch 920/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0714 - acc: 0.9944 - val_loss: 0.3700 - val_acc: 0.8778\n",
            "Epoch 921/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0716 - acc: 0.9930 - val_loss: 0.3701 - val_acc: 0.8722\n",
            "Epoch 922/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0716 - acc: 0.9958 - val_loss: 0.3742 - val_acc: 0.8722\n",
            "Epoch 923/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0703 - acc: 0.9958 - val_loss: 0.3740 - val_acc: 0.8778\n",
            "Epoch 924/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0702 - acc: 0.9944 - val_loss: 0.3713 - val_acc: 0.8833\n",
            "Epoch 925/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0706 - acc: 0.9958 - val_loss: 0.3797 - val_acc: 0.8722\n",
            "Epoch 926/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.0698 - acc: 0.9944 - val_loss: 0.3683 - val_acc: 0.8833\n",
            "Epoch 927/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0714 - acc: 0.9917 - val_loss: 0.3685 - val_acc: 0.8778\n",
            "Epoch 928/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0688 - acc: 0.9972 - val_loss: 0.3803 - val_acc: 0.8778\n",
            "Epoch 929/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0696 - acc: 0.9944 - val_loss: 0.3760 - val_acc: 0.8944\n",
            "Epoch 930/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0693 - acc: 0.9958 - val_loss: 0.3796 - val_acc: 0.8889\n",
            "Epoch 931/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.0688 - acc: 0.9958 - val_loss: 0.3678 - val_acc: 0.8722\n",
            "Epoch 932/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0684 - acc: 0.9930 - val_loss: 0.3815 - val_acc: 0.8778\n",
            "Epoch 933/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0674 - acc: 0.9972 - val_loss: 0.3771 - val_acc: 0.8944\n",
            "Epoch 934/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0676 - acc: 0.9944 - val_loss: 0.3610 - val_acc: 0.8889\n",
            "Epoch 935/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0671 - acc: 0.9958 - val_loss: 0.3709 - val_acc: 0.8944\n",
            "Epoch 936/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0683 - acc: 0.9930 - val_loss: 0.3805 - val_acc: 0.8833\n",
            "Epoch 937/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0669 - acc: 0.9972 - val_loss: 0.3695 - val_acc: 0.9000\n",
            "Epoch 938/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0660 - acc: 0.9958 - val_loss: 0.3758 - val_acc: 0.8944\n",
            "Epoch 939/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0672 - acc: 0.9958 - val_loss: 0.3692 - val_acc: 0.8722\n",
            "Epoch 940/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0651 - acc: 0.9972 - val_loss: 0.3579 - val_acc: 0.8833\n",
            "Epoch 941/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.0664 - acc: 0.9972 - val_loss: 0.3829 - val_acc: 0.8889\n",
            "Epoch 942/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0671 - acc: 0.9944 - val_loss: 0.3667 - val_acc: 0.8778\n",
            "Epoch 943/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0647 - acc: 0.9958 - val_loss: 0.3737 - val_acc: 0.8722\n",
            "Epoch 944/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0640 - acc: 0.9986 - val_loss: 0.3673 - val_acc: 0.8833\n",
            "Epoch 945/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0640 - acc: 0.9986 - val_loss: 0.3819 - val_acc: 0.8722\n",
            "Epoch 946/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0641 - acc: 0.9972 - val_loss: 0.3820 - val_acc: 0.8833\n",
            "Epoch 947/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.0642 - acc: 0.9972 - val_loss: 0.3622 - val_acc: 0.8833\n",
            "Epoch 948/1000\n",
            "719/719 [==============================] - 0s 47us/step - loss: 0.0632 - acc: 0.9972 - val_loss: 0.3800 - val_acc: 0.8833\n",
            "Epoch 949/1000\n",
            "719/719 [==============================] - 0s 57us/step - loss: 0.0631 - acc: 0.9958 - val_loss: 0.3682 - val_acc: 0.8944\n",
            "Epoch 950/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0630 - acc: 0.9972 - val_loss: 0.3735 - val_acc: 0.8667\n",
            "Epoch 951/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0631 - acc: 0.9958 - val_loss: 0.3725 - val_acc: 0.8722\n",
            "Epoch 952/1000\n",
            "719/719 [==============================] - 0s 58us/step - loss: 0.0626 - acc: 0.9944 - val_loss: 0.3688 - val_acc: 0.8722\n",
            "Epoch 953/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0620 - acc: 0.9944 - val_loss: 0.3626 - val_acc: 0.8722\n",
            "Epoch 954/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0622 - acc: 0.9944 - val_loss: 0.3595 - val_acc: 0.8944\n",
            "Epoch 955/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0613 - acc: 0.9958 - val_loss: 0.3658 - val_acc: 0.8778\n",
            "Epoch 956/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0620 - acc: 0.9958 - val_loss: 0.3730 - val_acc: 0.8722\n",
            "Epoch 957/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0614 - acc: 0.9972 - val_loss: 0.3780 - val_acc: 0.8944\n",
            "Epoch 958/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0608 - acc: 0.9972 - val_loss: 0.3647 - val_acc: 0.8833\n",
            "Epoch 959/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0603 - acc: 0.9958 - val_loss: 0.3760 - val_acc: 0.8944\n",
            "Epoch 960/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0610 - acc: 0.9972 - val_loss: 0.3651 - val_acc: 0.8667\n",
            "Epoch 961/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0607 - acc: 0.9972 - val_loss: 0.3603 - val_acc: 0.8778\n",
            "Epoch 962/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0611 - acc: 0.9986 - val_loss: 0.3660 - val_acc: 0.8778\n",
            "Epoch 963/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0598 - acc: 0.9958 - val_loss: 0.3640 - val_acc: 0.8889\n",
            "Epoch 964/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0590 - acc: 0.9972 - val_loss: 0.3732 - val_acc: 0.8944\n",
            "Epoch 965/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0593 - acc: 0.9986 - val_loss: 0.3698 - val_acc: 0.8722\n",
            "Epoch 966/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0589 - acc: 0.9972 - val_loss: 0.3635 - val_acc: 0.8667\n",
            "Epoch 967/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0582 - acc: 0.9986 - val_loss: 0.3645 - val_acc: 0.8778\n",
            "Epoch 968/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0583 - acc: 0.9972 - val_loss: 0.3623 - val_acc: 0.8667\n",
            "Epoch 969/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0586 - acc: 0.9986 - val_loss: 0.3616 - val_acc: 0.8944\n",
            "Epoch 970/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0574 - acc: 0.9972 - val_loss: 0.3561 - val_acc: 0.8889\n",
            "Epoch 971/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0583 - acc: 0.9986 - val_loss: 0.3704 - val_acc: 0.8944\n",
            "Epoch 972/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0574 - acc: 0.9972 - val_loss: 0.3520 - val_acc: 0.8944\n",
            "Epoch 973/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.0573 - acc: 0.9986 - val_loss: 0.3667 - val_acc: 0.8722\n",
            "Epoch 974/1000\n",
            "719/719 [==============================] - 0s 52us/step - loss: 0.0572 - acc: 0.9958 - val_loss: 0.3658 - val_acc: 0.8722\n",
            "Epoch 975/1000\n",
            "719/719 [==============================] - 0s 46us/step - loss: 0.0565 - acc: 0.9958 - val_loss: 0.3796 - val_acc: 0.8722\n",
            "Epoch 976/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0572 - acc: 0.9986 - val_loss: 0.3643 - val_acc: 0.8778\n",
            "Epoch 977/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0557 - acc: 0.9986 - val_loss: 0.3675 - val_acc: 0.8889\n",
            "Epoch 978/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0559 - acc: 0.9986 - val_loss: 0.3662 - val_acc: 0.8778\n",
            "Epoch 979/1000\n",
            "719/719 [==============================] - 0s 59us/step - loss: 0.0558 - acc: 0.9986 - val_loss: 0.3652 - val_acc: 0.8778\n",
            "Epoch 980/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0558 - acc: 0.9972 - val_loss: 0.3705 - val_acc: 0.8722\n",
            "Epoch 981/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0551 - acc: 0.9986 - val_loss: 0.3695 - val_acc: 0.8722\n",
            "Epoch 982/1000\n",
            "719/719 [==============================] - 0s 48us/step - loss: 0.0549 - acc: 0.9986 - val_loss: 0.3667 - val_acc: 0.9000\n",
            "Epoch 983/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0544 - acc: 0.9972 - val_loss: 0.3626 - val_acc: 0.8944\n",
            "Epoch 984/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0548 - acc: 0.9972 - val_loss: 0.3751 - val_acc: 0.8944\n",
            "Epoch 985/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0541 - acc: 0.9986 - val_loss: 0.3556 - val_acc: 0.8944\n",
            "Epoch 986/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0540 - acc: 0.9972 - val_loss: 0.3599 - val_acc: 0.8944\n",
            "Epoch 987/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0536 - acc: 0.9986 - val_loss: 0.3670 - val_acc: 0.9056\n",
            "Epoch 988/1000\n",
            "719/719 [==============================] - 0s 49us/step - loss: 0.0536 - acc: 0.9972 - val_loss: 0.3642 - val_acc: 0.8778\n",
            "Epoch 989/1000\n",
            "719/719 [==============================] - 0s 51us/step - loss: 0.0532 - acc: 0.9986 - val_loss: 0.3602 - val_acc: 0.8889\n",
            "Epoch 990/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0526 - acc: 0.9986 - val_loss: 0.3531 - val_acc: 0.8833\n",
            "Epoch 991/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0525 - acc: 0.9972 - val_loss: 0.3663 - val_acc: 0.8833\n",
            "Epoch 992/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0530 - acc: 0.9986 - val_loss: 0.3613 - val_acc: 0.8889\n",
            "Epoch 993/1000\n",
            "719/719 [==============================] - 0s 55us/step - loss: 0.0520 - acc: 0.9986 - val_loss: 0.3760 - val_acc: 0.8833\n",
            "Epoch 994/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0526 - acc: 0.9986 - val_loss: 0.3686 - val_acc: 0.9000\n",
            "Epoch 995/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0516 - acc: 0.9986 - val_loss: 0.3891 - val_acc: 0.8722\n",
            "Epoch 996/1000\n",
            "719/719 [==============================] - 0s 60us/step - loss: 0.0528 - acc: 0.9958 - val_loss: 0.3610 - val_acc: 0.8778\n",
            "Epoch 997/1000\n",
            "719/719 [==============================] - 0s 50us/step - loss: 0.0512 - acc: 0.9986 - val_loss: 0.3695 - val_acc: 0.8833\n",
            "Epoch 998/1000\n",
            "719/719 [==============================] - 0s 56us/step - loss: 0.0515 - acc: 0.9972 - val_loss: 0.3661 - val_acc: 0.9000\n",
            "Epoch 999/1000\n",
            "719/719 [==============================] - 0s 53us/step - loss: 0.0517 - acc: 0.9986 - val_loss: 0.3770 - val_acc: 0.8778\n",
            "Epoch 1000/1000\n",
            "719/719 [==============================] - 0s 54us/step - loss: 0.0515 - acc: 0.9986 - val_loss: 0.3708 - val_acc: 0.8722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rRcauj7trr3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loss Accuracy and time\n"
      ]
    },
    {
      "metadata": {
        "id": "l-yHvuvq04yV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "55b01859-5b77-470a-c924-f6dc3c136952"
      },
      "cell_type": "code",
      "source": [
        "#If Validation accuracy is lower than training accuracy, implies overfitting.\n",
        "\n",
        "\n",
        "#model.evaluate returns the loss value & metrics values for the model\n",
        "loss,acc=model.evaluate(processedTestData,processedTestLabel)\n",
        "print(\"Loss=\",loss,\"\\nAccuracy=\",acc)\n",
        "time_taken=end-start\n",
        "print(\"Time Taken=\",end-start)\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 0s 78us/step\n",
            "Loss= 0.34101722240447996 \n",
            "Accuracy= 0.82\n",
            "Time Taken= 41.25078225135803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QREkAQK1ox-I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Visualisation"
      ]
    },
    {
      "metadata": {
        "id": "5w1-w2_i5wu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "36bd3153-fab1-4bc3-c9d6-b1a9ec245a6f"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "#!pip install gspread\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "sh = gc.open_by_key('1gfvIG0NSh1N4tTfOMoXbMxprQP_n6QC_sG6Y3nrvZgI')\n",
        "# Open our new sheet and add some data.\n",
        "#worksheet = sh.worksheet(\"Sheet1\")\n",
        "#metric_list =[nodes_in_layer1,nodes_in_layer2,nodes_in_layer3,acc,loss,time_taken] \n",
        "#worksheet.append_row(metric_list, value_input_option='RAW')\n",
        "\n",
        "#worksheet = sh.worksheet(\"Activation_function\")\n",
        "#Activation_fn_metrics =[Activation_function.__name__,acc,loss,time_taken] \n",
        "#worksheet.append_row(Activation_fn_metrics, value_input_option='RAW')\n",
        "\n",
        "\n",
        "#worksheet = sh.worksheet(\"Loss_function\")\n",
        "#loss_fn_metrics =[loss_function,acc,loss,time_taken] \n",
        "#worksheet.append_row(loss_fn_metrics, value_input_option='RAW')\n",
        "\n",
        "\n",
        "worksheet = sh.worksheet(\"Optimizer\")\n",
        "Optimizer_metrics =[optimizer_used,acc,loss,time_taken] \n",
        "worksheet.append_row(Optimizer_metrics, value_input_option='RAW')\n",
        "'''"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#!pip install gspread\\nfrom google.colab import auth\\nauth.authenticate_user()\\nimport gspread\\nfrom oauth2client.client import GoogleCredentials\\ngc = gspread.authorize(GoogleCredentials.get_application_default())\\n\\nsh = gc.open_by_key(\\'1gfvIG0NSh1N4tTfOMoXbMxprQP_n6QC_sG6Y3nrvZgI\\')\\n# Open our new sheet and add some data.\\n#worksheet = sh.worksheet(\"Sheet1\")\\n#metric_list =[nodes_in_layer1,nodes_in_layer2,nodes_in_layer3,acc,loss,time_taken] \\n#worksheet.append_row(metric_list, value_input_option=\\'RAW\\')\\n\\n#worksheet = sh.worksheet(\"Activation_function\")\\n#Activation_fn_metrics =[Activation_function.__name__,acc,loss,time_taken] \\n#worksheet.append_row(Activation_fn_metrics, value_input_option=\\'RAW\\')\\n\\n\\n#worksheet = sh.worksheet(\"Loss_function\")\\n#loss_fn_metrics =[loss_function,acc,loss,time_taken] \\n#worksheet.append_row(loss_fn_metrics, value_input_option=\\'RAW\\')\\n\\n\\nworksheet = sh.worksheet(\"Optimizer\")\\nOptimizer_metrics =[optimizer_used,acc,loss,time_taken] \\nworksheet.append_row(Optimizer_metrics, value_input_option=\\'RAW\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "X0JRD6n5Q84G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Visualisation**\n",
        "\n",
        "Standard deviation to check the stability of the model's accuracy and loss\n"
      ]
    },
    {
      "metadata": {
        "id": "ChgBNwJFptsG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879
        },
        "outputId": "05bf6831-f9bf-4f5e-8289-81f069094672"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "%matplotlib inline\n",
        "df = pd.DataFrame(history.history)\n",
        "df.plot(subplots=True, grid=True, figsize=(10,15))\n",
        "print(np.std(df))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc         0.193957\n",
            "loss        0.424726\n",
            "val_acc     0.149805\n",
            "val_loss    0.327252\n",
            "dtype: float64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAMHCAYAAAAZ3AKAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2NVjHHsOf6QAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f4cd0b60630>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "o_bmWRiI_WIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "b7bd0f1a-6af8-4a69-b521-f2eb20db3e03"
      },
      "cell_type": "code",
      "source": [
        "#print(model.predict(processedTestData))\n",
        "#predictedTestLabel is the output labels of teset dataset as classes 0,1,2,3\n",
        "predictedTestLabel = []\n",
        "for i,j in zip(processedTestData,processedTestLabel):\n",
        "    y = model.predict(np.array(i).reshape(-1,10))\n",
        "    predictedTestLabel.append(y.argmax())\n",
        "\n",
        "\n",
        "#processedTestLabel2 for storing the target labels of test dataset as classes 0,1,2,3\n",
        "processedTestLabel2=[]\n",
        "for Label in processedTestLabel:\n",
        "  processedTestLabel2.append(Label.argmax())\n",
        "\n",
        "\n",
        "#Printing the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(processedTestLabel2,predictedTestLabel)\n",
        "print(cm)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3, 0, 3, 1, 0, 3, 3, 0, 0, 3, 0, 3, 3, 0, 3, 0, 0, 1, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 0, 1, 0, 3, 1, 0, 0, 3, 3, 3, 3, 2, 0, 3, 0, 1, 1, 0, 3, 0, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 0, 0, 0, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 3, 0, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 1, 0, 0]\n",
            "[3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1, 3, 0, 3, 3, 2, 3, 3, 0, 3, 1, 0, 3, 3, 0, 1]\n",
            "[[25  0  0  2]\n",
            " [ 5  9  0  0]\n",
            " [ 1  0  5  0]\n",
            " [ 5  5  0 43]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RQ4eA4B-wSHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Exporting the output.csv\n",
        "\n",
        "def decodeLabel(encodedLabel):\n",
        "    if encodedLabel == 0:\n",
        "        return \"Other\"\n",
        "    elif encodedLabel == 1:\n",
        "        return \"Fizz\"\n",
        "    elif encodedLabel == 2:\n",
        "        return \"Buzz\"\n",
        "    elif encodedLabel == 3:\n",
        "        return \"FizzBuzz\"\n",
        "\n",
        "predictedTestLabel2=[]\n",
        "for Label in predictedTestLabel:\n",
        "  predictedTestLabel2.append(decodeLabel(Label))\n",
        "\n",
        "processedTestLabel3=[]\n",
        "for Label in processedTestLabel2:\n",
        "  processedTestLabel2.append(decodeLabel(Label))\n",
        "  \n",
        "testDataInput = processedTestData\n",
        "testDataLabel = processedTestLabel3\n",
        "\n",
        "output = {}\n",
        "output[\"input\"] = testDataInput\n",
        "output[\"label\"] = testDataLabel\n",
        "\n",
        "output[\"predicted_label\"] = predictedTestLabel2\n",
        "\n",
        "print(processedTestLabel3)\n",
        "#opdf = pd.DataFrame(output)\n",
        "#opdf.to_csv('output.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}